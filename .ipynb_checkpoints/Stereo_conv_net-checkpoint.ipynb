{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lasagne.layers (slower)\n"
     ]
    }
   ],
   "source": [
    "import dicom, cv2, re, sys\n",
    "import os, fnmatch, shutil, subprocess\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import dicom,  cv2, re, sys\n",
    "import os, fnmatch, shutil, subprocess\n",
    "from IPython.utils import io\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "import matplotlib as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # we ignore a RuntimeWarning produced from dividing by zero\n",
    "import os, sys, urllib, gzip\n",
    "import cPickle as pickle\n",
    "sys.setrecursionlimit(10000)\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from IPython.display import Image as IPImage\n",
    "\n",
    "from PIL import Image\n",
    "from lasagne.layers import get_output, InputLayer, DenseLayer, Upscale2DLayer, ReshapeLayer\n",
    "from lasagne.init import GlorotUniform\n",
    "from lasagne.nonlinearities import rectify, leaky_rectify, tanh, sigmoid, softmax\n",
    "from lasagne.updates import nesterov_momentum, adam\n",
    "from lasagne.objectives import categorical_crossentropy, binary_crossentropy\n",
    "from nolearn.lasagne import NeuralNet, BatchIterator, PrintLayerInfo\n",
    "from lasagne.layers import Conv2DLayer as Conv2DLayer\n",
    "from lasagne.layers import MaxPool2DLayer as MaxPool2DLayer\n",
    "import theano \n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "import time\n",
    "try:\n",
    "    from lasagne.layers.dnn import Conv2DDNNLayer as Conv2DLayer\n",
    "    from lasagne.layers.dnn import MaxPool2DDNNLayer as MaxPool2DLayer\n",
    "    print 'Using cuda_convnet (faster)'\n",
    "except ImportError:\n",
    "    from lasagne.layers import Conv2DLayer as Conv2DLayer\n",
    "    from lasagne.layers import MaxPool2DLayer as MaxPool2DLayer\n",
    "    print 'Using lasagne.layers (slower)'\n",
    "    \n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from lasagne.layers import Layer\n",
    "from lasagne import init\n",
    "from lasagne import nonlinearities\n",
    "from scipy.misc import imresize, imread\n",
    "from PIL import ImageOps\n",
    "import scipy as sp\n",
    "import scipy.ndimage.morphology\n",
    "from skimage.morphology import convex_hull_image\n",
    "from skimage.restoration import denoise_tv_chambolle, denoise_bilateral\n",
    "import matplotlib.cm as cm\n",
    "from scipy.optimize import minimize\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_StereoImages(dirpath='/Users/louis/Documents/DepthMap_dataset'):\n",
    "    # load training data\n",
    "    X , X_left,X_right, y =[], [], [], []\n",
    "    for path in glob.glob('%s/Depth_map/DepthMap*' % dirpath):\n",
    "        with open(path, 'rb') as f:\n",
    "            depthM = imread(f)\n",
    "            size_depthM = depthM.shape\n",
    "            depthM = Image.open(f)\n",
    "            depthM = depthM.convert('L')\n",
    "            depthM = np.array(depthM.getdata(),dtype=np.uint8)\n",
    "        y.append(depthM)\n",
    "    y = np.concatenate(y).reshape(-1, 1, size_depthM[0], size_depthM[1]).astype(np.float32)\n",
    "    \n",
    "    for path in glob.glob('%s/StereoImages/Stereoscopic*' % dirpath):\n",
    "        with open(path, 'rb') as f:\n",
    "            StereoIm = imread(f)\n",
    "            size_StereoIm = StereoIm.shape\n",
    "            StereoIm = Image.open(f)\n",
    "            StereoIm = StereoIm.convert('L')\n",
    "            StereoIm = np.array(StereoIm.getdata(),dtype=np.uint8).reshape(1, size_StereoIm[0], size_StereoIm[1])\n",
    "            Im_left = StereoIm[0,...,0:size_StereoIm[1]/2]\n",
    "            Im_right = StereoIm[0,...,size_StereoIm[1]/2:size_StereoIm[1]]\n",
    "#             StereoIm = StereoIm.reshape(-1,1)\n",
    "        X.append(StereoIm)\n",
    "        X_left.append(Im_left)\n",
    "        X_right.append(Im_right)\n",
    "        \n",
    "    X = np.concatenate((X_left,X_right),1).reshape(-1, 2, size_StereoIm[0], size_StereoIm[1]/2).astype(np.float32)\n",
    "    X_left = np.concatenate(X_left).reshape(-1, 1, size_StereoIm[0], size_StereoIm[1]/2).astype(np.float32)\n",
    "    X_right = np.concatenate(X_right).reshape(-1, 1, size_StereoIm[0], size_StereoIm[1]/2).astype(np.float32)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print X_left.shape, X_right.shape, X.shape\n",
    "    \n",
    "    ii = np.random.permutation(len(X))\n",
    "    X_train = X[ii[floor(len(X)*0.1):]]\n",
    "    X_left_train = X_left[ii[floor(len(X_left)*0.1):]]\n",
    "    X_right_train = X_right[ii[floor(len(X_right)*0.1):]]\n",
    "    y_train = y[ii[floor(len(X)*0.1):]]\n",
    "    \n",
    "    X_valid = X[ii[:floor(len(X)*0.1)]]\n",
    "    X_left_valid = X_left[ii[:floor(len(X_left)*0.1)]]\n",
    "    X_right_valid = X_right[ii[:floor(len(X_right)*0.1)]]\n",
    "    y_valid = y[ii[:floor(len(X)*0.1)]]\n",
    "    \n",
    "    # normalize to zero mean and unity variance\n",
    "    offset = np.mean(X_train, 0)\n",
    "    scale = np.std(X_train, 0).clip(min=1)\n",
    "    \n",
    "    offset_left = np.mean(X_left_train, 0)\n",
    "    scale_left = np.std(X_left_train, 0).clip(min=1)\n",
    "    offset_right = np.mean(X_right_train, 0)\n",
    "    scale_right = np.std(X_right_train, 0).clip(min=1)\n",
    "    \n",
    "    X_train = (X_train - offset) / scale\n",
    "    X_valid = (X_valid - offset) / scale\n",
    "    \n",
    "    X_left_train = (X_left_train - offset_left) / scale_left\n",
    "    X_right_train = (X_right_train - offset_right) / scale_right\n",
    "\n",
    "    X_left_valid = (X_left_valid - offset_left) / scale_left\n",
    "    X_right_valid = (X_right_valid - offset_right) / scale_right\n",
    "    return X_train,X_left_train,X_right_train, y_train, X_valid,X_left_valid,X_right_valid, y_valid\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 1, 200, 300) (500, 1, 200, 300) (500, 2, 200, 300)\n",
      "(450, 2, 200, 300) (450, 1, 200, 300) (50, 2, 200, 300) (50, 1, 200, 300)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_left_train,X_right_train, y_train, X_valid,X_left_valid,X_right_valid, y_valid = load_StereoImages()\n",
    "print X_train.shape, y_train.shape, X_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEACAYAAACuzv3DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nHch32mmnnS6cdiDfaaeddrpw+n9aAwc+iEXoewAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x123801690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# y_train[11,0,...].shape\n",
    "plt.imshow(y_train[11,0,...],cmap = cm.Greys_r)\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(X_valid[14,0,...],cmap = cm.Greys_r)\n",
    "plt.show()\n",
    "# plt.imshow(X_train[11,1,...],cmap = cm.Greys_r)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from batchNormalization import BatchNormLayer\n",
    "\n",
    "def batch_norm(layer, **kwargs):\n",
    "    nonlinearity = getattr(layer, 'nonlinearity', None)\n",
    "    if nonlinearity is not None:\n",
    "        layer.nonlinearity = nonlinearities.identity\n",
    "    if hasattr(layer, 'b') and layer.b is not None:\n",
    "        del layer.params[layer.b]\n",
    "        layer.b = None\n",
    "    layer = BatchNormLayer(layer, **kwargs)\n",
    "    if nonlinearity is not None:\n",
    "        from lasagne.layers import NonlinearityLayer\n",
    "        layer = NonlinearityLayer(layer, nonlinearity)\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_stereo_cnn(input_var=None):\n",
    "    \n",
    "    conv_num_filters1 = 16\n",
    "    conv_num_filters2 = 32\n",
    "    conv_num_filters3 = 32\n",
    "    conv_num_filters4 = 32\n",
    "    filter_size1 = 7\n",
    "    filter_size2 = 5\n",
    "    filter_size3 = 3\n",
    "    filter_size4 = 3\n",
    "    pool_size = 2\n",
    "    scale_factor = 2\n",
    "    pad_in = 'valid'\n",
    "    pad_out = 'full'\n",
    "\n",
    "    # Input layer, as usual:                                                                                                                                                                                \n",
    "    network = InputLayer(shape=(None,2, X_train.shape[2], X_train.shape[3]),input_var=input_var,name=\"input_layer\")                                                                                                                             \n",
    "        \n",
    "    network = batch_norm(Conv2DLayer(\n",
    "            network, num_filters=conv_num_filters1, filter_size=(filter_size1, filter_size1),pad=pad_in,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"conv1\"))\n",
    "    \n",
    "    network = MaxPool2DLayer(network, pool_size=(pool_size, pool_size),name=\"pool1\")\n",
    "    \n",
    "    network = batch_norm(Conv2DLayer(\n",
    "            network, num_filters=conv_num_filters2, filter_size=(filter_size2, filter_size2),pad=pad_in,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"conv2\"))\n",
    "    \n",
    "    network = MaxPool2DLayer(network, pool_size=(pool_size, pool_size),name=\"pool2\")\n",
    "                                                                                                                                     \n",
    "    network = batch_norm(Conv2DLayer(\n",
    "            network, num_filters=conv_num_filters3, filter_size=(filter_size3, filter_size3),pad=pad_in,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"conv3\"))\n",
    "    \n",
    "    network = MaxPool2DLayer(network, pool_size=(pool_size, pool_size),name=\"pool3\")\n",
    "                                                                                                                                     \n",
    "    network = batch_norm(Conv2DLayer(\n",
    "            network, num_filters=conv_num_filters4, filter_size=(filter_size4, filter_size4),pad=pad_in,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"conv4\"))\n",
    "    \n",
    "    network = batch_norm(Conv2DLayer(\n",
    "            network, num_filters=16, filter_size=(filter_size4, filter_size4),pad=pad_out,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"deconv1\"))\n",
    "    \n",
    "    network = Upscale2DLayer(network, scale_factor=(pool_size, pool_size),name=\"upscale1\")\n",
    "    \n",
    "    network = batch_norm(Conv2DLayer(\n",
    "            network, num_filters=8, filter_size=(filter_size3, filter_size3),pad=pad_out,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"deconv2\"))\n",
    "    \n",
    "    network = Upscale2DLayer(network, scale_factor=(pool_size, pool_size),name=\"upscale2\")\n",
    "    \n",
    "    network = batch_norm(Conv2DLayer(\n",
    "            network, num_filters=4, filter_size=(filter_size2, filter_size2),pad=pad_out,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"deconv3\"))\n",
    "    \n",
    "    network = Upscale2DLayer(network, scale_factor=(pool_size, pool_size),name=\"upscale3\")\n",
    "    \n",
    "    network = batch_norm(Conv2DLayer(\n",
    "            network, num_filters=1, filter_size=(filter_size1, filter_size1),pad=pad_out,\n",
    "            nonlinearity=lasagne.nonlinearities.sigmoid,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"deconv4\"))\n",
    "                                 \n",
    "    return network\n",
    "\n",
    "def build_cnn2(input_var_left=None,input_var_right=None):\n",
    "    \n",
    "    conv_num_filters1 = 16\n",
    "    conv_num_filters2 = 32\n",
    "    conv_num_filters3 = 32\n",
    "    conv_num_filters4 = 32\n",
    "    filter_size1 = 7\n",
    "    filter_size2 = 5\n",
    "    filter_size3 = 3\n",
    "    filter_size4 = 3\n",
    "    pool_size = 2\n",
    "    scale_factor = 2\n",
    "    pad_in = 'valid'\n",
    "    pad_out = 'full'\n",
    "\n",
    "    # Input layers, left and right:                                                                                                                                                                                \n",
    "    input_layer_left = InputLayer(shape=(None,1, X_left_train.shape[2], X_left_train.shape[3]),input_var=input_var_left,name=\"input_layer_left\")   \n",
    "    \n",
    "    input_layer_right = InputLayer(shape=(None,1, X_right_train.shape[2], X_right_train.shape[3]),input_var=input_var_right,name=\"input_layer_right\")   \n",
    "        \n",
    "    \n",
    "    # First Conv layer with batch normalization, left and right:  \n",
    "    Conv1_left = batch_norm(Conv2DLayer(\n",
    "            input_layer_left, num_filters=conv_num_filters1, filter_size=(filter_size1, filter_size1),pad=pad_in,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"conv1_left\"))\n",
    "    \n",
    "    Conv1_right = batch_norm(Conv2DLayer(\n",
    "            input_layer_right, num_filters=conv_num_filters1, filter_size=(filter_size1, filter_size1),pad=pad_in,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"conv1_right\"))\n",
    "    \n",
    "    # First max pool layer, left and right:  \n",
    "    \n",
    "    MaxPool1_left = MaxPool2DLayer(Conv1_left, pool_size=(pool_size, pool_size),name=\"pool1_left\")\n",
    "    \n",
    "    MaxPool1_right = MaxPool2DLayer(Conv1_right, pool_size=(pool_size, pool_size),name=\"pool1_right\")\n",
    "    \n",
    "    \n",
    "    # Second Conv layer with batch normalization, left and right: # Second Conv layer with batch normalization, left and right:  \n",
    "    \n",
    "    Conv2_left = batch_norm(Conv2DLayer(\n",
    "            Conv1_left, num_filters=conv_num_filters2, filter_size=(filter_size2, filter_size2),pad=pad_in,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"conv2_left\"))\n",
    "    \n",
    "    Conv2_right = batch_norm(Conv2DLayer(\n",
    "            Conv1_right, num_filters=conv_num_filters2, filter_size=(filter_size2, filter_size2),pad=pad_in,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"conv2_right\"))\n",
    "    \n",
    "    # Second max pool layer, left and right:  \n",
    "    \n",
    "    MaxPool2_left = MaxPool2DLayer(Conv2_left, pool_size=(pool_size, pool_size),name=\"pool2_left\")\n",
    "    \n",
    "    MaxPool2_right = MaxPool2DLayer(Conv2_right, pool_size=(pool_size, pool_size),name=\"pool2_right\")\n",
    "    \n",
    "    \n",
    "    # Third Conv layer with batch normalization, left and right: \n",
    "    \n",
    "    Conv3_left = batch_norm(Conv2DLayer(\n",
    "            MaxPool2_left, num_filters=conv_num_filters3, filter_size=(filter_size3, filter_size3),pad=pad_in,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"conv3_left\"))\n",
    "    \n",
    "    Conv3_right = batch_norm(Conv2DLayer(\n",
    "            MaxPool2_right, num_filters=conv_num_filters3, filter_size=(filter_size3, filter_size3),pad=pad_in,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"conv3_right\"))\n",
    "    \n",
    "    # Third max pool layer, left and right:  \n",
    "    \n",
    "    MaxPool3_left = MaxPool2DLayer(Conv3_left, pool_size=(pool_size, pool_size),name=\"pool3_left\")\n",
    "    \n",
    "    MaxPool3_right = MaxPool2DLayer(Conv3_right, pool_size=(pool_size, pool_size),name=\"pool3_right\")\n",
    "    \n",
    "    \n",
    "    # Fourth and final downward Conv layer with batch normalization, left and right: \n",
    "                                                                                                                                     \n",
    "    Conv4_left = batch_norm(Conv2DLayer(\n",
    "            MaxPool3_left, num_filters=conv_num_filters4, filter_size=(filter_size4, filter_size4),pad=pad_in,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"conv4_left\"))\n",
    "    \n",
    "    Conv4_right = batch_norm(Conv2DLayer(\n",
    "            MaxPool3_right, num_filters=conv_num_filters4, filter_size=(filter_size4, filter_size4),pad=pad_in,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"conv4_right\"))\n",
    "    \n",
    "    # concatenate layer, join left and right images:\n",
    "    \n",
    "    ConcatLayer = ConcatLayer([Conv4_left,Conv4_right])\n",
    "    \n",
    "    network = batch_norm(Conv2DLayer(\n",
    "            ConcatLayer, num_filters=16, filter_size=(filter_size4, filter_size4),pad=pad_out,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"deconv1\"))\n",
    "    \n",
    "    network = Upscale2DLayer(network, scale_factor=(pool_size, pool_size),name=\"upscale1\")\n",
    "    \n",
    "    network = batch_norm(Conv2DLayer(\n",
    "            network, num_filters=8, filter_size=(filter_size3, filter_size3),pad=pad_out,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"deconv2\"))\n",
    "    \n",
    "    network = Upscale2DLayer(network, scale_factor=(pool_size, pool_size),name=\"upscale2\")\n",
    "    \n",
    "    network = batch_norm(Conv2DLayer(\n",
    "            network, num_filters=4, filter_size=(filter_size2, filter_size2),pad=pad_out,\n",
    "            nonlinearity=lasagne.nonlinearities.rectify,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"deconv3\"))\n",
    "    \n",
    "    network = Upscale2DLayer(network, scale_factor=(pool_size, pool_size),name=\"upscale3\")\n",
    "    \n",
    "    network = batch_norm(Conv2DLayer(\n",
    "            network, num_filters=1, filter_size=(filter_size1, filter_size1),pad=pad_out,\n",
    "            nonlinearity=lasagne.nonlinearities.sigmoid,\n",
    "            W=lasagne.init.GlorotUniform(),name=\"deconv4\"))\n",
    "                                 \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]\n",
    "\n",
    "def iterator(X, batchsize):\n",
    "    indices = np.arange(len(X))\n",
    "    for i in range(0, len(X) - batchsize + 1, batchsize):\n",
    "        sli = indices[i:i+batchsize]\n",
    "        yield X[sli]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_params(model, fn):\n",
    "    with open(fn, 'w') as wr:\n",
    "        pickle.dump(lasagne.layers.get_all_param_values(model), wr)\n",
    "        \n",
    "def dice_loss(predictions, targets):\n",
    "    dice_index = 2*T.sum(predictions*targets)/((T.sum(predictions)+T.sum(targets)))\n",
    "    return -dice_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(model='cnn', num_epochs=200):\n",
    "    # Load the dataset                                                                                                                                                                                      \n",
    "    print(\"Loading data...\")\n",
    "    X_train,X_left_train,X_right_train, y_train, X_val,X_left_val,X_right_val, y_val = load_StereoImages()\n",
    "    \n",
    "    indices = np.arange(len(X_train))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    y_train = y_train[...,1:y_train.shape[2]-1,3:y_train.shape[3]-3]\n",
    "    y_val = y_val[...,1:y_val.shape[2]-1,3:y_val.shape[3]-3]\n",
    "    \n",
    "    print 'X_train type and shape:', X_train.dtype, X_train.shape\n",
    "    print 'X_train.min():', X_train.min()\n",
    "    print 'X_train.max():', X_train.max()\n",
    "\n",
    "    print 'X_val type and shape:', X_val.dtype, X_val.shape\n",
    "    print 'X_val.min():', X_val.min()\n",
    "    print 'X_val.max():', X_val.max()\n",
    "    \n",
    "    print 'y_train type and shape:', y_train.dtype, y_train.shape\n",
    "    print 'y_train.min():', y_train.min()\n",
    "    print 'y_train.max():', y_train.max()\n",
    "\n",
    "    print 'y_val type and shape:', y_val.dtype, y_val.shape\n",
    "    print 'y_val.min():', y_val.min()\n",
    "    print 'y_val.max():', y_val.max()\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets                                                                                                                                                       \n",
    "    input_var = T.tensor4('inputs',dtype=theano.config.floatX)\n",
    "    target_var = T.tensor4('targets',dtype=theano.config.floatX)\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)                                                                                                                               \n",
    "    print(\"Building model and compiling functions...\")\n",
    "\n",
    "    network = build_stereo_cnn(input_var)\n",
    "    laylist = lasagne.layers.get_all_layers(network)\n",
    "    \n",
    "    for l in laylist:\n",
    "        print l.name, lasagne.layers.get_output_shape(l)\n",
    "        \n",
    "    # Create a loss expression for training, i.e., a scalar objective we want\n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    dloss = lasagne.objectives.squared_error(prediction, target_var)\n",
    "    dloss = dloss.mean()\n",
    "#     loss = lasagne.objectives.binary_crossentropy(prediction, target_var)\n",
    "#     loss = loss.mean()\n",
    "    \n",
    "#     dloss = lasagne.objectives.squared_error(prediction, target_var)\n",
    "#     dloss = dloss.mean()\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.\n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the\n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient\n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "#     updates = lasagne.updates.adam(\n",
    "#             loss, params, learning_rate=0.01)\n",
    "    updates2 = lasagne.updates.adam(\n",
    "            dloss, params, learning_rate=0.01)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference\n",
    "    # here is that we do a deterministic forward pass through the network,\n",
    "    # disabling dropout layers.\n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "#     test_loss = lasagne.objectives.binary_crossentropy(test_prediction,\n",
    "#                                                             target_var)\n",
    "#     test_loss = test_loss.mean()\n",
    "    \n",
    "    test_loss2 = lasagne.objectives.squared_error(test_prediction,target_var)\n",
    "    test_loss2 = test_loss2.mean()\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving\n",
    "    # the updates dictionary) and returning the corresponding training loss:\n",
    "#     train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "    \n",
    "    train_fn2 = theano.function([input_var, target_var], dloss, updates=updates2)\n",
    "\n",
    "    # Compile a second function computing the validation loss and accuracy:\n",
    "#     val_fn = theano.function([input_var, target_var], [test_loss])\n",
    "    \n",
    "    val_fn2 = theano.function([input_var, target_var], [test_loss2])\n",
    "\n",
    "    # Finally, launch the training loop.\n",
    "    print(\"Starting training...\")\n",
    "    # We iterate over epochs:                                                                                                                                                                               \n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "        for batch in iterate_minibatches(X_train, y_train, 16, shuffle=True):\n",
    "            inputs, targets = batch\n",
    "            train_err += train_fn2(inputs, targets)\n",
    "            train_batches += 1\n",
    "#             print train_fn2(inputs, targets)\n",
    "        # And a full pass over the validation data:\n",
    "        val_err = 0\n",
    "        val_batches = 0\n",
    "        for batch in iterate_minibatches(X_val, y_val, 16, shuffle=False):\n",
    "            inputs, targets = batch\n",
    "            err = val_fn2(inputs, targets)\n",
    "            val_err += err[0]\n",
    "            val_batches += 1\n",
    "\n",
    "        # Then we print the results for this epoch:                                                                                                                                                         \n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        print(\"  ratio:\\t\\t{:.6f}\".format((train_err / train_batches)/(val_err / val_batches)))\n",
    "#         filename = \"model4_loss:\\t\\t{:.6f}\".format(train_err / train_batches)\n",
    "#         np.savez(filename, *lasagne.layers.get_all_param_values(network)) \n",
    "        np.savez('model_stereo.npz', *lasagne.layers.get_all_param_values(network)) \n",
    "        !git add model_stereo.npz\n",
    "        !git commit -m \"new stereo model\"\n",
    "        !git push -u origin master\n",
    "        \n",
    "    # After training, we compute and print the test error:                                                                                                                                                  \n",
    "#     test_err = 0\n",
    "#     test_acc = 0\n",
    "#     test_batches = 0\n",
    "#     for batch in iterator(X_test,32):\n",
    "#         err = val_fn(batch)\n",
    "#         test_err += err\n",
    "#         test_batches += 1\n",
    "        \n",
    "#     print(\"Final results:\")\n",
    "#     print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    \n",
    "    # Optionally, you could now dump the network weights to a file like this:                                                                                                                               \n",
    "    np.savez('model_stereo.npz', *lasagne.layers.get_all_param_values(network))  \n",
    "    !git add model_stereo.npz\n",
    "    !git commit -m \"new stereo model\"\n",
    "    !git push -u origin master\n",
    "    #                                                                                                                                                                                                       \n",
    "    # And load them again later on like this:                                                                                                                                                               \n",
    "    # with np.load('model.npz') as f:                                                                                                                                                                       \n",
    "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]                                                                                                                                     \n",
    "    # lasagne.layers.set_all_param_values(network, param_values)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(500, 1, 200, 300) (500, 1, 200, 300) (500, 2, 200, 300)\n",
      "(450, 1, 200, 300)\n",
      "X_train type and shape: float32 (450, 2, 200, 300)\n",
      "X_train.min(): -7.25551\n",
      "X_train.max(): 5.45157\n",
      "X_val type and shape: float32 (50, 2, 200, 300)\n",
      "X_val.min(): -8.05825\n",
      "X_val.max(): -3.27868\n",
      "y_train type and shape: float32 (450, 1, 198, 294)\n",
      "y_train.min(): "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO (theano.gof.compilelock): Refreshing lock /Users/louis/.theano/compiledir_Darwin-15.2.0-x86_64-i386-64bit-i386-2.7.10-64/lock_dir/lock\n",
      "INFO:theano.gof.compilelock:Refreshing lock /Users/louis/.theano/compiledir_Darwin-15.2.0-x86_64-i386-64bit-i386-2.7.10-64/lock_dir/lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "y_train.max(): 227.0\n",
      "y_val type and shape: float32 (50, 1, 198, 294)\n",
      "y_val.min(): 0.0\n",
      "y_val.max(): 225.0\n",
      "Building model and compiling functions...\n",
      "input_layer (None, 2, 200, 300)\n",
      "conv1 (None, 16, 194, 294)\n",
      "None (None, 16, 194, 294)\n",
      "None (None, 16, 194, 294)\n",
      "pool1 (None, 16, 97, 147)\n",
      "conv2 (None, 32, 93, 143)\n",
      "None (None, 32, 93, 143)\n",
      "None (None, 32, 93, 143)\n",
      "pool2 (None, 32, 46, 71)\n",
      "conv3 (None, 32, 44, 69)\n",
      "None (None, 32, 44, 69)\n",
      "None (None, 32, 44, 69)\n",
      "pool3 (None, 32, 22, 34)\n",
      "conv4 (None, 32, 20, 32)\n",
      "None (None, 32, 20, 32)\n",
      "None (None, 32, 20, 32)\n",
      "deconv1 (None, 16, 22, 34)\n",
      "None (None, 16, 22, 34)\n",
      "None (None, 16, 22, 34)\n",
      "upscale1 (None, 16, 44, 68)\n",
      "deconv2 (None, 8, 46, 70)\n",
      "None (None, 8, 46, 70)\n",
      "None (None, 8, 46, 70)\n",
      "upscale2 (None, 8, 92, 140)\n",
      "deconv3 (None, 4, 96, 144)\n",
      "None (None, 4, 96, 144)\n",
      "None (None, 4, 96, 144)\n",
      "upscale3 (None, 4, 192, 288)\n",
      "deconv4 (None, 1, 198, 294)\n",
      "None (None, 1, 198, 294)\n",
      "None (None, 1, 198, 294)\n",
      "Starting training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-228-89c95f559ca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cnn'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-227-6c6e9978c9a4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(model, num_epochs)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterate_minibatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_fn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mtrain_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;31m#             print train_fn2(inputs, targets)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(model='cnn', num_epochs=45)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 1, 200, 300)\n"
     ]
    }
   ],
   "source": [
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(model='cnn', num_epochs=500):\n",
    "    # Load the dataset                                                                                                                                                                                      \n",
    "    print(\"Loading data...\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "\n",
    "    print('Train data shape: ', X_train.shape)\n",
    "    print('Train labels shape: ', y_train.shape)\n",
    "    print('Validation data shape: ', X_val.shape)\n",
    "    print('Validation labels shape: ', y_val.shape)\n",
    "    print('Test data shape: ', X_test.shape)\n",
    "    print('Test labels shape: ', y_test.shape)\n",
    "\n",
    "    # Prepare Theano variables for inputs and targets                                                                                                                                                       \n",
    "    input_var = T.tensor4('inputs')\n",
    "    #target_var = T.ivector('targets')\n",
    "\n",
    "    # Create neural network model (depending on first command line parameter)                                                                                                                               \n",
    "    print(\"Building model and compiling functions...\")\n",
    "\n",
    "    network = build_cae1(input_var)\n",
    "    laylist = lasagne.layers.get_all_layers(network)\n",
    "    \n",
    "    with np.load('model.npz') as f:                                                                                                                                                                       \n",
    "        param_values = [f['arr_%d' % i] for i in range(len(f.files))]  \n",
    "        \n",
    "    lasagne.layers.set_all_param_values(network, param_values)  \n",
    "    \n",
    "    for l in laylist:\n",
    "        print l.name, lasagne.layers.get_output_shape(l)\n",
    "        \n",
    "    # Create a loss expression for training, i.e., a scalar objective we want                                                                                                                               \n",
    "    # to minimize (for our multi-class problem, it is the cross-entropy loss):                                                                                                                              \n",
    "    prediction = lasagne.layers.get_output(network)\n",
    "    loss = lasagne.objectives.squared_error(prediction, input_var.flatten(2))\n",
    "    loss = loss.mean()\n",
    "    # We could add some weight decay as well here, see lasagne.regularization.                                                                                                                              \n",
    "\n",
    "    # Create update expressions for training, i.e., how to modify the                                                                                                                                       \n",
    "    # parameters at each training step. Here, we'll use Stochastic Gradient                                                                                                                                 \n",
    "    # Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.                                                                                                                                 \n",
    "    params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "    updates = lasagne.updates.nesterov_momentum(\n",
    "            loss, params, learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "    # Create a loss expression for validation/testing. The crucial difference                                                                                                                               \n",
    "    # here is that we do a deterministic forward pass through the network,                                                                                                                                  \n",
    "    # disabling dropout layers.                                                                                                                                                                             \n",
    "    test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "    test_loss = lasagne.objectives.squared_error(test_prediction,\n",
    "                                                            input_var.flatten(2))\n",
    "    test_loss = test_loss.mean()\n",
    "    # As a bonus, also create an expression for the classification accuracy:                                                                                                                                \n",
    "    #test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), input_var.flatten(2)),dtype=theano.config.floatX)\n",
    "\n",
    "    # Compile a function performing a training step on a mini-batch (by giving                                                                                                                              \n",
    "    # the updates dictionary) and returning the corresponding training loss:                                                                                                                                \n",
    "    #train_fn = theano.function([input_var, target_var], loss, updates=updates)\n",
    "    train_fn = theano.function([input_var], loss, updates=updates)\n",
    "    #out_fn = theano.function([input_var],prediction)\n",
    "    #loss_fn = theano.function([input_var],loss)\n",
    "    # Compile a second function computing the validation loss and accuracy:                                                                                                                                 \n",
    "    #val_fn = theano.function([input_var, target_var], [test_loss, test_acc])\n",
    "    val_fn = theano.function([input_var], test_loss)\n",
    "    # Finally, launch the training loop.                                                                                                                                                                    \n",
    "    print(\"Starting training...\")\n",
    "    loss_history = []\n",
    "    # We iterate over epochs:                                                                                                                                                                               \n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:                                                                                                                                          \n",
    "        train_err = 0\n",
    "        train_batches = 0\n",
    "        start_time = time.time()\n",
    "            \n",
    "        for batch in iterator(X_train, 500):\n",
    "            train_err += train_fn(batch)\n",
    "            train_batches += 1\n",
    "            loss_history.append(train_err)\n",
    "        \n",
    "        # And a full pass over the validation data:                                                                                                                                                         \n",
    "        val_err = 0\n",
    "        val_acc = 0\n",
    "        val_batches = 0\n",
    "        \n",
    "        for batch in iterator(X_val, 500):\n",
    "            err = val_fn(batch)\n",
    "            val_err += err\n",
    "            val_batches += 1\n",
    "            \n",
    "\n",
    "        # Then we print the results for this epoch:                                                                                                                                                         \n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
    "        print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
    "        np.savez('model.npz', *lasagne.layers.get_all_param_values(network))           \n",
    "        \n",
    "    # After training, we compute and print the test error:                                                                                                                                                  \n",
    "    test_err = 0\n",
    "    test_acc = 0\n",
    "    test_batches = 0\n",
    "    for batch in iterator(X_test,500):\n",
    "        err = val_fn(batch)\n",
    "        test_err += err\n",
    "        test_batches += 1\n",
    "        \n",
    "    print(\"Final results:\")\n",
    "    print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
    "    \n",
    "    # Optionally, you could now dump the network weights to a file like this:                                                                                                                               \n",
    "    np.savez('model.npz', *lasagne.layers.get_all_param_values(network))                                                                                                                                  \n",
    "    #                                                                                                                                                                                                       \n",
    "    # And load them again later on like this:                                                                                                                                                               \n",
    "    # with np.load('model.npz') as f:                                                                                                                                                                       \n",
    "    #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]                                                                                                                                     \n",
    "    # lasagne.layers.set_all_param_values(network, param_values)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "('Train data shape: ', (49000, 3, 32, 32))\n",
      "('Train labels shape: ', (49000,))\n",
      "('Validation data shape: ', (1000, 3, 32, 32))\n",
      "('Validation labels shape: ', (1000,))\n",
      "('Test data shape: ', (10000, 3, 32, 32))\n",
      "('Test labels shape: ', (10000,))\n",
      "Building model and compiling functions...\n",
      "InputLayer_0 (None, 3, 32, 32)\n",
      "Conv2DLayer_1 (None, 32, 28, 28)\n",
      "MaxPool2DLayer_3 (None, 32, 14, 14)\n",
      "ReshapeLayer_6 (None, 6272)\n",
      "dense_mid_7 (None, 100)\n",
      "dense_mid_10 (None, 6272)\n",
      "ReshapeLayer_11 (None, 32, 14, 14)\n",
      "Upscale2DLayer_12 (None, 32, 28, 28)\n",
      "Conv2DLayer_13 (None, 3, 32, 32)\n",
      "ReshapeLayer_17 (None, 3072)\n",
      "Starting training...\n",
      "Epoch 1 of 500 took 13.356s\n",
      "  training loss:\t\t0.525950\n",
      "  validation loss:\t\t0.526529\n",
      "Epoch 2 of 500 took 13.307s\n",
      "  training loss:\t\t0.525731\n",
      "  validation loss:\t\t0.526744\n",
      "Epoch 3 of 500 took 13.267s\n",
      "  training loss:\t\t0.525639\n",
      "  validation loss:\t\t0.526725\n",
      "Epoch 4 of 500 took 13.290s\n",
      "  training loss:\t\t0.525660\n",
      "  validation loss:\t\t0.526639\n",
      "Epoch 5 of 500 took 13.283s\n",
      "  training loss:\t\t0.525601\n",
      "  validation loss:\t\t0.526683\n",
      "Epoch 6 of 500 took 13.293s\n",
      "  training loss:\t\t0.525587\n",
      "  validation loss:\t\t0.526622\n",
      "Epoch 7 of 500 took 13.283s\n",
      "  training loss:\t\t0.525541\n",
      "  validation loss:\t\t0.526659\n",
      "Epoch 8 of 500 took 13.276s\n",
      "  training loss:\t\t0.525530\n",
      "  validation loss:\t\t0.526567\n",
      "Epoch 9 of 500 took 13.277s\n",
      "  training loss:\t\t0.525475\n",
      "  validation loss:\t\t0.526621\n",
      "Epoch 10 of 500 took 13.284s\n",
      "  training loss:\t\t0.525479\n",
      "  validation loss:\t\t0.526504\n",
      "Epoch 11 of 500 took 13.271s\n",
      "  training loss:\t\t0.525404\n",
      "  validation loss:\t\t0.526589\n",
      "Epoch 12 of 500 took 13.274s\n",
      "  training loss:\t\t0.525439\n",
      "  validation loss:\t\t0.526435\n",
      "Epoch 13 of 500 took 13.272s\n",
      "  training loss:\t\t0.525317\n",
      "  validation loss:\t\t0.526545\n",
      "Epoch 14 of 500 took 13.279s\n",
      "  training loss:\t\t0.525414\n",
      "  validation loss:\t\t0.526305\n",
      "Epoch 15 of 500 took 13.279s\n",
      "  training loss:\t\t0.525201\n",
      "  validation loss:\t\t0.526483\n",
      "Epoch 16 of 500 took 13.267s\n",
      "  training loss:\t\t0.525403\n",
      "  validation loss:\t\t0.526096\n",
      "Epoch 17 of 500 took 13.275s\n",
      "  training loss:\t\t0.525083\n",
      "  validation loss:\t\t0.526355\n",
      "Epoch 18 of 500 took 13.303s\n",
      "  training loss:\t\t0.525396\n",
      "  validation loss:\t\t0.525825\n",
      "Epoch 19 of 500 took 13.277s\n",
      "  training loss:\t\t0.525154\n",
      "  validation loss:\t\t0.528653\n",
      "Epoch 20 of 500 took 13.286s\n",
      "  training loss:\t\t0.525258\n",
      "  validation loss:\t\t0.526046\n",
      "Epoch 21 of 500 took 13.278s\n",
      "  training loss:\t\t0.525231\n",
      "  validation loss:\t\t0.526341\n",
      "Epoch 22 of 500 took 13.272s\n",
      "  training loss:\t\t0.525086\n",
      "  validation loss:\t\t0.526289\n",
      "Epoch 23 of 500 took 13.278s\n",
      "  training loss:\t\t0.525131\n",
      "  validation loss:\t\t0.526145\n",
      "Epoch 24 of 500 took 13.272s\n",
      "  training loss:\t\t0.525080\n",
      "  validation loss:\t\t0.526362\n",
      "Epoch 25 of 500 took 13.277s\n",
      "  training loss:\t\t0.525017\n",
      "  validation loss:\t\t0.525900\n",
      "Epoch 26 of 500 took 13.277s\n",
      "  training loss:\t\t0.525118\n",
      "  validation loss:\t\t0.526490\n",
      "Epoch 27 of 500 took 13.280s\n",
      "  training loss:\t\t0.524882\n",
      "  validation loss:\t\t0.525749\n",
      "Epoch 28 of 500 took 13.311s\n",
      "  training loss:\t\t0.525323\n",
      "  validation loss:\t\t0.525624\n",
      "Epoch 29 of 500 took 13.310s\n",
      "  training loss:\t\t0.525014\n",
      "  validation loss:\t\t0.526224\n",
      "Epoch 30 of 500 took 13.269s\n",
      "  training loss:\t\t0.524880\n",
      "  validation loss:\t\t0.526099\n",
      "Epoch 31 of 500 took 13.263s\n",
      "  training loss:\t\t0.524976\n",
      "  validation loss:\t\t0.525975\n",
      "Epoch 32 of 500 took 13.280s\n",
      "  training loss:\t\t0.524856\n",
      "  validation loss:\t\t0.526152\n",
      "Epoch 33 of 500 took 13.305s\n",
      "  training loss:\t\t0.524910\n",
      "  validation loss:\t\t0.525977\n",
      "Epoch 34 of 500 took 13.335s\n",
      "  training loss:\t\t0.524801\n",
      "  validation loss:\t\t0.526095\n",
      "Epoch 35 of 500 took 13.326s\n",
      "  training loss:\t\t0.524889\n",
      "  validation loss:\t\t0.525931\n",
      "Epoch 36 of 500 took 13.327s\n",
      "  training loss:\t\t0.524712\n",
      "  validation loss:\t\t0.526020\n",
      "Epoch 37 of 500 took 13.295s\n",
      "  training loss:\t\t0.524881\n",
      "  validation loss:\t\t0.525758\n",
      "Epoch 38 of 500 took 13.279s\n",
      "  training loss:\t\t0.524587\n",
      "  validation loss:\t\t0.525812\n",
      "Epoch 39 of 500 took 13.273s\n",
      "  training loss:\t\t0.524897\n",
      "  validation loss:\t\t0.525505\n",
      "Epoch 40 of 500 took 13.282s\n",
      "  training loss:\t\t0.524432\n",
      "  validation loss:\t\t0.525754\n",
      "Epoch 41 of 500 took 13.341s\n",
      "  training loss:\t\t0.525046\n",
      "  validation loss:\t\t0.525578\n",
      "Epoch 42 of 500 took 13.309s\n",
      "  training loss:\t\t0.524724\n",
      "  validation loss:\t\t0.526610\n",
      "Epoch 43 of 500 took 13.308s\n",
      "  training loss:\t\t0.524530\n",
      "  validation loss:\t\t0.525360\n",
      "Epoch 44 of 500 took 13.302s\n",
      "  training loss:\t\t0.524792\n",
      "  validation loss:\t\t0.526318\n",
      "Epoch 45 of 500 took 13.305s\n",
      "  training loss:\t\t0.524446\n",
      "  validation loss:\t\t0.525182\n",
      "Epoch 46 of 500 took 13.270s\n",
      "  training loss:\t\t0.524854\n",
      "  validation loss:\t\t0.525585\n",
      "Epoch 47 of 500 took 13.275s\n",
      "  training loss:\t\t0.524477\n",
      "  validation loss:\t\t0.525773\n",
      "Epoch 48 of 500 took 13.277s\n",
      "  training loss:\t\t0.524620\n",
      "  validation loss:\t\t0.525603\n",
      "Epoch 49 of 500 took 13.283s\n",
      "  training loss:\t\t0.524494\n",
      "  validation loss:\t\t0.525806\n",
      "Epoch 50 of 500 took 13.315s\n",
      "  training loss:\t\t0.524532\n",
      "  validation loss:\t\t0.525654\n",
      "Epoch 51 of 500 took 13.316s\n",
      "  training loss:\t\t0.524469\n",
      "  validation loss:\t\t0.525744\n",
      "Epoch 52 of 500 took 13.275s\n",
      "  training loss:\t\t0.524491\n",
      "  validation loss:\t\t0.525642\n",
      "Epoch 53 of 500 took 13.263s\n",
      "  training loss:\t\t0.524405\n",
      "  validation loss:\t\t0.525660\n",
      "Epoch 54 of 500 took 13.265s\n",
      "  training loss:\t\t0.524499\n",
      "  validation loss:\t\t0.525575\n",
      "Epoch 55 of 500 took 13.306s\n",
      "  training loss:\t\t0.524234\n",
      "  validation loss:\t\t0.525200\n",
      "Epoch 56 of 500 took 13.320s\n",
      "  training loss:\t\t0.524624\n",
      "  validation loss:\t\t0.525499\n",
      "Epoch 57 of 500 took 13.309s\n",
      "  training loss:\t\t0.524306\n",
      "  validation loss:\t\t0.527423\n",
      "Epoch 58 of 500 took 13.286s\n",
      "  training loss:\t\t0.524499\n",
      "  validation loss:\t\t0.525819\n",
      "Epoch 59 of 500 took 13.289s\n",
      "  training loss:\t\t0.524380\n",
      "  validation loss:\t\t0.525887\n",
      "Epoch 60 of 500 took 13.270s\n",
      "  training loss:\t\t0.524299\n",
      "  validation loss:\t\t0.525732\n",
      "Epoch 61 of 500 took 13.304s\n",
      "  training loss:\t\t0.524332\n",
      "  validation loss:\t\t0.525763\n",
      "Epoch 62 of 500 took 13.293s\n",
      "  training loss:\t\t0.524253\n",
      "  validation loss:\t\t0.525663\n",
      "Epoch 63 of 500 took 13.285s\n",
      "  training loss:\t\t0.524303\n",
      "  validation loss:\t\t0.525763\n",
      "Epoch 64 of 500 took 13.315s\n",
      "  training loss:\t\t0.524173\n",
      "  validation loss:\t\t0.525374\n",
      "Epoch 65 of 500 took 13.280s\n",
      "  training loss:\t\t0.524327\n",
      "  validation loss:\t\t0.525773\n",
      "Epoch 66 of 500 took 13.276s\n",
      "  training loss:\t\t0.524060\n",
      "  validation loss:\t\t0.524854\n",
      "Epoch 67 of 500 took 13.271s\n",
      "  training loss:\t\t0.524602\n",
      "  validation loss:\t\t0.525260\n",
      "Epoch 68 of 500 took 13.276s\n",
      "  training loss:\t\t0.523959\n",
      "  validation loss:\t\t0.524943\n",
      "Epoch 69 of 500 took 13.264s\n",
      "  training loss:\t\t0.524271\n",
      "  validation loss:\t\t0.525222\n",
      "Epoch 70 of 500 took 13.269s\n",
      "  training loss:\t\t0.524103\n",
      "  validation loss:\t\t0.525249\n",
      "Epoch 71 of 500 took 13.285s\n",
      "  training loss:\t\t0.524150\n",
      "  validation loss:\t\t0.525224\n",
      "Epoch 72 of 500 took 13.276s\n",
      "  training loss:\t\t0.524087\n",
      "  validation loss:\t\t0.525264\n",
      "Epoch 73 of 500 took 13.268s\n",
      "  training loss:\t\t0.524102\n",
      "  validation loss:\t\t0.525209\n",
      "Epoch 74 of 500 took 13.270s\n",
      "  training loss:\t\t0.524050\n",
      "  validation loss:\t\t0.525258\n",
      "Epoch 75 of 500 took 13.279s\n",
      "  training loss:\t\t0.524076\n",
      "  validation loss:\t\t0.525185\n",
      "Epoch 76 of 500 took 13.265s\n",
      "  training loss:\t\t0.524002\n",
      "  validation loss:\t\t0.525244\n",
      "Epoch 77 of 500 took 13.279s\n",
      "  training loss:\t\t0.524060\n",
      "  validation loss:\t\t0.525099\n",
      "Epoch 78 of 500 took 13.270s\n",
      "  training loss:\t\t0.523933\n",
      "  validation loss:\t\t0.525260\n",
      "Epoch 79 of 500 took 13.273s\n",
      "  training loss:\t\t0.524061\n",
      "  validation loss:\t\t0.524961\n",
      "Epoch 80 of 500 took 13.264s\n",
      "  training loss:\t\t0.523815\n",
      "  validation loss:\t\t0.525033\n",
      "Epoch 81 of 500 took 13.271s\n",
      "  training loss:\t\t0.524109\n",
      "  validation loss:\t\t0.524737\n",
      "Epoch 82 of 500 took 13.270s\n",
      "  training loss:\t\t0.524177\n",
      "  validation loss:\t\t0.526391\n",
      "Epoch 83 of 500 took 13.272s\n",
      "  training loss:\t\t0.523859\n",
      "  validation loss:\t\t0.525255\n",
      "Epoch 84 of 500 took 13.270s\n",
      "  training loss:\t\t0.523980\n",
      "  validation loss:\t\t0.525391\n",
      "Epoch 85 of 500 took 13.268s\n",
      "  training loss:\t\t0.523882\n",
      "  validation loss:\t\t0.525372\n",
      "Epoch 86 of 500 took 13.274s\n",
      "  training loss:\t\t0.523903\n",
      "  validation loss:\t\t0.525353\n",
      "Epoch 87 of 500 took 13.308s\n",
      "  training loss:\t\t0.523851\n",
      "  validation loss:\t\t0.525292\n",
      "Epoch 88 of 500 took 13.274s\n",
      "  training loss:\t\t0.523878\n",
      "  validation loss:\t\t0.525376\n",
      "Epoch 89 of 500 took 13.269s\n",
      "  training loss:\t\t0.523789\n",
      "  validation loss:\t\t0.525130\n",
      "Epoch 90 of 500 took 13.277s\n",
      "  training loss:\t\t0.523887\n",
      "  validation loss:\t\t0.525407\n",
      "Epoch 91 of 500 took 13.274s\n",
      "  training loss:\t\t0.523680\n",
      "  validation loss:\t\t0.524657\n",
      "Epoch 92 of 500 took 13.271s\n",
      "  training loss:\t\t0.523993\n",
      "  validation loss:\t\t0.525622\n",
      "Epoch 93 of 500 took 13.268s\n",
      "  training loss:\t\t0.523703\n",
      "  validation loss:\t\t0.525487\n",
      "Epoch 94 of 500 took 13.272s\n",
      "  training loss:\t\t0.523911\n",
      "  validation loss:\t\t0.524829\n",
      "Epoch 95 of 500 took 13.280s\n",
      "  training loss:\t\t0.523736\n",
      "  validation loss:\t\t0.525069\n",
      "Epoch 96 of 500 took 13.288s\n",
      "  training loss:\t\t0.523716\n",
      "  validation loss:\t\t0.524901\n",
      "Epoch 97 of 500 took 13.298s\n",
      "  training loss:\t\t0.523727\n",
      "  validation loss:\t\t0.524955\n",
      "Epoch 98 of 500 took 13.280s\n",
      "  training loss:\t\t0.523664\n",
      "  validation loss:\t\t0.524856\n",
      "Epoch 99 of 500 took 13.275s\n",
      "  training loss:\t\t0.523724\n",
      "  validation loss:\t\t0.524961\n",
      "Epoch 100 of 500 took 13.282s\n",
      "  training loss:\t\t0.523595\n",
      "  validation loss:\t\t0.524754\n",
      "Epoch 101 of 500 took 13.277s\n",
      "  training loss:\t\t0.523747\n",
      "  validation loss:\t\t0.524967\n",
      "Epoch 102 of 500 took 13.271s\n",
      "  training loss:\t\t0.523487\n",
      "  validation loss:\t\t0.524458\n",
      "Epoch 103 of 500 took 13.283s\n",
      "  training loss:\t\t0.523859\n",
      "  validation loss:\t\t0.525102\n",
      "Epoch 104 of 500 took 13.299s\n",
      "  training loss:\t\t0.523456\n",
      "  validation loss:\t\t0.525615\n",
      "Epoch 105 of 500 took 13.273s\n",
      "  training loss:\t\t0.523811\n",
      "  validation loss:\t\t0.525093\n",
      "Epoch 106 of 500 took 13.268s\n",
      "  training loss:\t\t0.523566\n",
      "  validation loss:\t\t0.525221\n",
      "Epoch 107 of 500 took 13.267s\n",
      "  training loss:\t\t0.523580\n",
      "  validation loss:\t\t0.525077\n",
      "Epoch 108 of 500 took 13.274s\n",
      "  training loss:\t\t0.523561\n",
      "  validation loss:\t\t0.525131\n",
      "Epoch 109 of 500 took 13.266s\n",
      "  training loss:\t\t0.523520\n",
      "  validation loss:\t\t0.524918\n",
      "Epoch 110 of 500 took 13.316s\n",
      "  training loss:\t\t0.523579\n",
      "  validation loss:\t\t0.525216\n",
      "Epoch 111 of 500 took 13.279s\n",
      "  training loss:\t\t0.523401\n",
      "  validation loss:\t\t0.524380\n",
      "Epoch 112 of 500 took 13.266s\n",
      "  training loss:\t\t0.523719\n",
      "  validation loss:\t\t0.525504\n",
      "Epoch 113 of 500 took 13.276s\n",
      "  training loss:\t\t0.523405\n",
      "  validation loss:\t\t0.525067\n",
      "Epoch 114 of 500 took 13.265s\n",
      "  training loss:\t\t0.523614\n",
      "  validation loss:\t\t0.524564\n",
      "Epoch 115 of 500 took 13.272s\n",
      "  training loss:\t\t0.523454\n",
      "  validation loss:\t\t0.524801\n",
      "Epoch 116 of 500 took 13.264s\n",
      "  training loss:\t\t0.523425\n",
      "  validation loss:\t\t0.524605\n",
      "Epoch 117 of 500 took 13.277s\n",
      "  training loss:\t\t0.523469\n",
      "  validation loss:\t\t0.524760\n",
      "Epoch 118 of 500 took 13.278s\n",
      "  training loss:\t\t0.523355\n",
      "  validation loss:\t\t0.524493\n",
      "Epoch 119 of 500 took 13.274s\n",
      "  training loss:\t\t0.523483\n",
      "  validation loss:\t\t0.524777\n",
      "Epoch 120 of 500 took 13.273s\n",
      "  training loss:\t\t0.523283\n",
      "  validation loss:\t\t0.524328\n",
      "Epoch 121 of 500 took 13.271s\n",
      "  training loss:\t\t0.523535\n",
      "  validation loss:\t\t0.524929\n",
      "Epoch 122 of 500 took 13.299s\n",
      "  training loss:\t\t0.523182\n",
      "  validation loss:\t\t0.524254\n",
      "Epoch 123 of 500 took 13.272s\n",
      "  training loss:\t\t0.523617\n",
      "  validation loss:\t\t0.524790\n",
      "Epoch 124 of 500 took 13.277s\n",
      "  training loss:\t\t0.523304\n",
      "  validation loss:\t\t0.524992\n",
      "Epoch 125 of 500 took 13.269s\n",
      "  training loss:\t\t0.523362\n",
      "  validation loss:\t\t0.524856\n",
      "Epoch 126 of 500 took 13.280s\n",
      "  training loss:\t\t0.523294\n",
      "  validation loss:\t\t0.524767\n",
      "Epoch 127 of 500 took 13.263s\n",
      "  training loss:\t\t0.523343\n",
      "  validation loss:\t\t0.524966\n",
      "Epoch 128 of 500 took 13.275s\n",
      "  training loss:\t\t0.523211\n",
      "  validation loss:\t\t0.524360\n",
      "Epoch 129 of 500 took 13.280s\n",
      "  training loss:\t\t0.523424\n",
      "  validation loss:\t\t0.525217\n",
      "Epoch 130 of 500 took 13.264s\n",
      "  training loss:\t\t0.523165\n",
      "  validation loss:\t\t0.524542\n",
      "Epoch 131 of 500 took 13.278s\n",
      "  training loss:\t\t0.523462\n",
      "  validation loss:\t\t0.524310\n",
      "Epoch 132 of 500 took 13.264s\n",
      "  training loss:\t\t0.523222\n",
      "  validation loss:\t\t0.524662\n",
      "Epoch 133 of 500 took 13.311s\n",
      "  training loss:\t\t0.523228\n",
      "  validation loss:\t\t0.524418\n",
      "Epoch 134 of 500 took 13.273s\n",
      "  training loss:\t\t0.523234\n",
      "  validation loss:\t\t0.524524\n",
      "Epoch 135 of 500 took 13.279s\n",
      "  training loss:\t\t0.523174\n",
      "  validation loss:\t\t0.524362\n",
      "Epoch 136 of 500 took 13.275s\n",
      "  training loss:\t\t0.523230\n",
      "  validation loss:\t\t0.524556\n",
      "Epoch 137 of 500 took 13.277s\n",
      "  training loss:\t\t0.523119\n",
      "  validation loss:\t\t0.524220\n",
      "Epoch 138 of 500 took 13.273s\n",
      "  training loss:\t\t0.523255\n",
      "  validation loss:\t\t0.524759\n",
      "Epoch 139 of 500 took 13.274s\n",
      "  training loss:\t\t0.523026\n",
      "  validation loss:\t\t0.523951\n",
      "Epoch 140 of 500 took 13.283s\n",
      "  training loss:\t\t0.523488\n",
      "  validation loss:\t\t0.524842\n",
      "Epoch 141 of 500 took 13.273s\n",
      "  training loss:\t\t0.522952\n",
      "  validation loss:\t\t0.524030\n",
      "Epoch 142 of 500 took 13.278s\n",
      "  training loss:\t\t0.523245\n",
      "  validation loss:\t\t0.525011\n",
      "Epoch 143 of 500 took 13.269s\n",
      "  training loss:\t\t0.523002\n",
      "  validation loss:\t\t0.524020\n",
      "Epoch 144 of 500 took 13.273s\n",
      "  training loss:\t\t0.523234\n",
      "  validation loss:\t\t0.525156\n",
      "Epoch 145 of 500 took 13.278s\n",
      "  training loss:\t\t0.522963\n",
      "  validation loss:\t\t0.523919\n",
      "Epoch 146 of 500 took 13.269s\n",
      "  training loss:\t\t0.523305\n",
      "  validation loss:\t\t0.524562\n",
      "Epoch 147 of 500 took 13.272s\n",
      "  training loss:\t\t0.522941\n",
      "  validation loss:\t\t0.524055\n",
      "Epoch 148 of 500 took 13.270s\n",
      "  training loss:\t\t0.523152\n",
      "  validation loss:\t\t0.524598\n",
      "Epoch 149 of 500 took 13.278s\n",
      "  training loss:\t\t0.522939\n",
      "  validation loss:\t\t0.524001\n",
      "Epoch 150 of 500 took 13.283s\n",
      "  training loss:\t\t0.523146\n",
      "  validation loss:\t\t0.524714\n",
      "Epoch 151 of 500 took 13.272s\n",
      "  training loss:\t\t0.522876\n",
      "  validation loss:\t\t0.523829\n",
      "Epoch 152 of 500 took 13.266s\n",
      "  training loss:\t\t0.523308\n",
      "  validation loss:\t\t0.524866\n",
      "Epoch 153 of 500 took 13.279s\n",
      "  training loss:\t\t0.522830\n",
      "  validation loss:\t\t0.523879\n",
      "Epoch 154 of 500 took 13.282s\n",
      "  training loss:\t\t0.523114\n",
      "  validation loss:\t\t0.525033\n",
      "Epoch 155 of 500 took 13.267s\n",
      "  training loss:\t\t0.522864\n",
      "  validation loss:\t\t0.523827\n",
      "Epoch 156 of 500 took 13.304s\n",
      "  training loss:\t\t0.522762\n",
      "  validation loss:\t\t0.524532\n",
      "Epoch 157 of 500 took 13.264s\n",
      "  training loss:\t\t0.523204\n",
      "  validation loss:\t\t0.524030\n",
      "Epoch 158 of 500 took 13.277s\n",
      "  training loss:\t\t0.522823\n",
      "  validation loss:\t\t0.524583\n",
      "Epoch 159 of 500 took 13.268s\n",
      "  training loss:\t\t0.523051\n",
      "  validation loss:\t\t0.524317\n",
      "Epoch 160 of 500 took 13.270s\n",
      "  training loss:\t\t0.522833\n",
      "  validation loss:\t\t0.524195\n",
      "Epoch 161 of 500 took 13.268s\n",
      "  training loss:\t\t0.523030\n",
      "  validation loss:\t\t0.524656\n",
      "Epoch 162 of 500 took 13.271s\n",
      "  training loss:\t\t0.522798\n",
      "  validation loss:\t\t0.524057\n",
      "Epoch 163 of 500 took 13.266s\n",
      "  training loss:\t\t0.523179\n",
      "  validation loss:\t\t0.524038\n",
      "Epoch 164 of 500 took 13.274s\n",
      "  training loss:\t\t0.522869\n",
      "  validation loss:\t\t0.524335\n",
      "Epoch 165 of 500 took 13.268s\n",
      "  training loss:\t\t0.522884\n",
      "  validation loss:\t\t0.524153\n",
      "Epoch 166 of 500 took 13.271s\n",
      "  training loss:\t\t0.522867\n",
      "  validation loss:\t\t0.524219\n",
      "Epoch 167 of 500 took 13.291s\n",
      "  training loss:\t\t0.522840\n",
      "  validation loss:\t\t0.524101\n",
      "Epoch 168 of 500 took 13.264s\n",
      "  training loss:\t\t0.522859\n",
      "  validation loss:\t\t0.524248\n",
      "Epoch 169 of 500 took 13.269s\n",
      "  training loss:\t\t0.522797\n",
      "  validation loss:\t\t0.524010\n",
      "Epoch 170 of 500 took 13.269s\n",
      "  training loss:\t\t0.522862\n",
      "  validation loss:\t\t0.524340\n",
      "Epoch 171 of 500 took 13.268s\n",
      "  training loss:\t\t0.522731\n",
      "  validation loss:\t\t0.523794\n",
      "Epoch 172 of 500 took 13.267s\n",
      "  training loss:\t\t0.522916\n",
      "  validation loss:\t\t0.524568\n",
      "Epoch 173 of 500 took 13.269s\n",
      "  training loss:\t\t0.522643\n",
      "  validation loss:\t\t0.523682\n",
      "Epoch 174 of 500 took 13.275s\n",
      "  training loss:\t\t0.523031\n",
      "  validation loss:\t\t0.524613\n",
      "Epoch 175 of 500 took 13.266s\n",
      "  training loss:\t\t0.522670\n",
      "  validation loss:\t\t0.523911\n",
      "Epoch 176 of 500 took 13.280s\n",
      "  training loss:\t\t0.522846\n",
      "  validation loss:\t\t0.524608\n",
      "Epoch 177 of 500 took 13.264s\n",
      "  training loss:\t\t0.522662\n",
      "  validation loss:\t\t0.523788\n",
      "Epoch 178 of 500 took 13.272s\n",
      "  training loss:\t\t0.522840\n",
      "  validation loss:\t\t0.524926\n",
      "Epoch 179 of 500 took 13.302s\n",
      "  training loss:\t\t0.522626\n",
      "  validation loss:\t\t0.523631\n",
      "Epoch 180 of 500 took 13.278s\n",
      "  training loss:\t\t0.522939\n",
      "  validation loss:\t\t0.524158\n",
      "Epoch 181 of 500 took 13.267s\n",
      "  training loss:\t\t0.522628\n",
      "  validation loss:\t\t0.523842\n",
      "Epoch 182 of 500 took 13.269s\n",
      "  training loss:\t\t0.522758\n",
      "  validation loss:\t\t0.524109\n",
      "Epoch 183 of 500 took 13.273s\n",
      "  training loss:\t\t0.522634\n",
      "  validation loss:\t\t0.523806\n",
      "Epoch 184 of 500 took 13.278s\n",
      "  training loss:\t\t0.522731\n",
      "  validation loss:\t\t0.524187\n",
      "Epoch 185 of 500 took 13.285s\n",
      "  training loss:\t\t0.522597\n",
      "  validation loss:\t\t0.523658\n",
      "Epoch 186 of 500 took 13.270s\n",
      "  training loss:\t\t0.522745\n",
      "  validation loss:\t\t0.524502\n",
      "Epoch 187 of 500 took 13.276s\n",
      "  training loss:\t\t0.522535\n",
      "  validation loss:\t\t0.523522\n",
      "Epoch 188 of 500 took 13.275s\n",
      "  training loss:\t\t0.522929\n",
      "  validation loss:\t\t0.524532\n",
      "Epoch 189 of 500 took 13.283s\n",
      "  training loss:\t\t0.522491\n",
      "  validation loss:\t\t0.523633\n",
      "Epoch 190 of 500 took 13.268s\n",
      "  training loss:\t\t0.522716\n",
      "  validation loss:\t\t0.524695\n",
      "Epoch 191 of 500 took 13.275s\n",
      "  training loss:\t\t0.522527\n",
      "  validation loss:\t\t0.523610\n",
      "Epoch 192 of 500 took 13.276s\n",
      "  training loss:\t\t0.522668\n",
      "  validation loss:\t\t0.524933\n",
      "Epoch 193 of 500 took 13.273s\n",
      "  training loss:\t\t0.522519\n",
      "  validation loss:\t\t0.523480\n",
      "Epoch 194 of 500 took 13.283s\n",
      "  training loss:\t\t0.522821\n",
      "  validation loss:\t\t0.524359\n",
      "Epoch 195 of 500 took 13.269s\n",
      "  training loss:\t\t0.522443\n",
      "  validation loss:\t\t0.523514\n",
      "Epoch 196 of 500 took 13.272s\n",
      "  training loss:\t\t0.522665\n",
      "  validation loss:\t\t0.524227\n",
      "Epoch 197 of 500 took 13.275s\n",
      "  training loss:\t\t0.522469\n",
      "  validation loss:\t\t0.523519\n",
      "Epoch 198 of 500 took 13.272s\n",
      "  training loss:\t\t0.522616\n",
      "  validation loss:\t\t0.524314\n",
      "Epoch 199 of 500 took 13.276s\n",
      "  training loss:\t\t0.522458\n",
      "  validation loss:\t\t0.523455\n",
      "Epoch 200 of 500 took 13.269s\n",
      "  training loss:\t\t0.522453\n",
      "  validation loss:\t\t0.524531\n",
      "Epoch 201 of 500 took 13.278s\n",
      "  training loss:\t\t0.522634\n",
      "  validation loss:\t\t0.523543\n",
      "Epoch 202 of 500 took 13.308s\n",
      "  training loss:\t\t0.522406\n",
      "  validation loss:\t\t0.523971\n",
      "Epoch 203 of 500 took 13.285s\n",
      "  training loss:\t\t0.522619\n",
      "  validation loss:\t\t0.523631\n",
      "Epoch 204 of 500 took 13.269s\n",
      "  training loss:\t\t0.522351\n",
      "  validation loss:\t\t0.523523\n",
      "Epoch 205 of 500 took 13.275s\n",
      "  training loss:\t\t0.522684\n",
      "  validation loss:\t\t0.524111\n",
      "Epoch 206 of 500 took 13.274s\n",
      "  training loss:\t\t0.522361\n",
      "  validation loss:\t\t0.524247\n",
      "Epoch 207 of 500 took 13.270s\n",
      "  training loss:\t\t0.522705\n",
      "  validation loss:\t\t0.524108\n",
      "Epoch 208 of 500 took 13.270s\n",
      "  training loss:\t\t0.522431\n",
      "  validation loss:\t\t0.524038\n",
      "Epoch 209 of 500 took 13.268s\n",
      "  training loss:\t\t0.522484\n",
      "  validation loss:\t\t0.524117\n",
      "Epoch 210 of 500 took 13.269s\n",
      "  training loss:\t\t0.522417\n",
      "  validation loss:\t\t0.523920\n",
      "Epoch 211 of 500 took 13.269s\n",
      "  training loss:\t\t0.522463\n",
      "  validation loss:\t\t0.524167\n",
      "Epoch 212 of 500 took 13.280s\n",
      "  training loss:\t\t0.522372\n",
      "  validation loss:\t\t0.523719\n",
      "Epoch 213 of 500 took 13.274s\n",
      "  training loss:\t\t0.522486\n",
      "  validation loss:\t\t0.524376\n",
      "Epoch 214 of 500 took 13.267s\n",
      "  training loss:\t\t0.522310\n",
      "  validation loss:\t\t0.523358\n",
      "Epoch 215 of 500 took 13.262s\n",
      "  training loss:\t\t0.522433\n",
      "  validation loss:\t\t0.525177\n",
      "Epoch 216 of 500 took 13.274s\n",
      "  training loss:\t\t0.522488\n",
      "  validation loss:\t\t0.523520\n",
      "Epoch 217 of 500 took 13.270s\n",
      "  training loss:\t\t0.522402\n",
      "  validation loss:\t\t0.523786\n",
      "Epoch 218 of 500 took 13.263s\n",
      "  training loss:\t\t0.522347\n",
      "  validation loss:\t\t0.523507\n",
      "Epoch 219 of 500 took 13.265s\n",
      "  training loss:\t\t0.522383\n",
      "  validation loss:\t\t0.523799\n",
      "Epoch 220 of 500 took 13.264s\n",
      "  training loss:\t\t0.522311\n",
      "  validation loss:\t\t0.523398\n",
      "Epoch 221 of 500 took 13.281s\n",
      "  training loss:\t\t0.522366\n",
      "  validation loss:\t\t0.524118\n",
      "Epoch 222 of 500 took 13.273s\n",
      "  training loss:\t\t0.522292\n",
      "  validation loss:\t\t0.523269\n",
      "Epoch 223 of 500 took 13.286s\n",
      "  training loss:\t\t0.522324\n",
      "  validation loss:\t\t0.525675\n",
      "Epoch 224 of 500 took 13.264s\n",
      "  training loss:\t\t0.522479\n",
      "  validation loss:\t\t0.523845\n",
      "Epoch 225 of 500 took 13.296s\n",
      "  training loss:\t\t0.522336\n",
      "  validation loss:\t\t0.524025\n",
      "Epoch 226 of 500 took 13.271s\n",
      "  training loss:\t\t0.522280\n",
      "  validation loss:\t\t0.523753\n",
      "Epoch 227 of 500 took 13.266s\n",
      "  training loss:\t\t0.522332\n",
      "  validation loss:\t\t0.524064\n",
      "Epoch 228 of 500 took 13.274s\n",
      "  training loss:\t\t0.522231\n",
      "  validation loss:\t\t0.523549\n",
      "Epoch 229 of 500 took 13.269s\n",
      "  training loss:\t\t0.522352\n",
      "  validation loss:\t\t0.524337\n",
      "Epoch 230 of 500 took 13.282s\n",
      "  training loss:\t\t0.522174\n",
      "  validation loss:\t\t0.523200\n",
      "Epoch 231 of 500 took 13.274s\n",
      "  training loss:\t\t0.522511\n",
      "  validation loss:\t\t0.523985\n",
      "Epoch 232 of 500 took 13.264s\n",
      "  training loss:\t\t0.522138\n",
      "  validation loss:\t\t0.523222\n",
      "Epoch 233 of 500 took 13.277s\n",
      "  training loss:\t\t0.522313\n",
      "  validation loss:\t\t0.524008\n",
      "Epoch 234 of 500 took 13.275s\n",
      "  training loss:\t\t0.522179\n",
      "  validation loss:\t\t0.523216\n",
      "Epoch 235 of 500 took 13.276s\n",
      "  training loss:\t\t0.522233\n",
      "  validation loss:\t\t0.524119\n",
      "Epoch 236 of 500 took 13.266s\n",
      "  training loss:\t\t0.522219\n",
      "  validation loss:\t\t0.523202\n",
      "Epoch 237 of 500 took 13.273s\n",
      "  training loss:\t\t0.522082\n",
      "  validation loss:\t\t0.523661\n",
      "Epoch 238 of 500 took 13.264s\n",
      "  training loss:\t\t0.522359\n",
      "  validation loss:\t\t0.523293\n",
      "Epoch 239 of 500 took 13.277s\n",
      "  training loss:\t\t0.522043\n",
      "  validation loss:\t\t0.523210\n",
      "Epoch 240 of 500 took 13.268s\n",
      "  training loss:\t\t0.522394\n",
      "  validation loss:\t\t0.523953\n",
      "Epoch 241 of 500 took 13.264s\n",
      "  training loss:\t\t0.522069\n",
      "  validation loss:\t\t0.523613\n",
      "Epoch 242 of 500 took 13.271s\n",
      "  training loss:\t\t0.522416\n",
      "  validation loss:\t\t0.523889\n",
      "Epoch 243 of 500 took 13.269s\n",
      "  training loss:\t\t0.522117\n",
      "  validation loss:\t\t0.523677\n",
      "Epoch 244 of 500 took 13.268s\n",
      "  training loss:\t\t0.522208\n",
      "  validation loss:\t\t0.523965\n",
      "Epoch 245 of 500 took 13.272s\n",
      "  training loss:\t\t0.522101\n",
      "  validation loss:\t\t0.523504\n",
      "Epoch 246 of 500 took 13.274s\n",
      "  training loss:\t\t0.522199\n",
      "  validation loss:\t\t0.524098\n",
      "Epoch 247 of 500 took 13.276s\n",
      "  training loss:\t\t0.522057\n",
      "  validation loss:\t\t0.523241\n",
      "Epoch 248 of 500 took 13.317s\n",
      "  training loss:\t\t0.522152\n",
      "  validation loss:\t\t0.524527\n",
      "Epoch 249 of 500 took 13.283s\n",
      "  training loss:\t\t0.522087\n",
      "  validation loss:\t\t0.523189\n",
      "Epoch 250 of 500 took 13.270s\n",
      "  training loss:\t\t0.521905\n",
      "  validation loss:\t\t0.523060\n",
      "Epoch 251 of 500 took 13.275s\n",
      "  training loss:\t\t0.522478\n",
      "  validation loss:\t\t0.523789\n",
      "Epoch 252 of 500 took 13.278s\n",
      "  training loss:\t\t0.521980\n",
      "  validation loss:\t\t0.523067\n",
      "Epoch 253 of 500 took 13.267s\n",
      "  training loss:\t\t0.522157\n",
      "  validation loss:\t\t0.524072\n",
      "Epoch 254 of 500 took 13.283s\n",
      "  training loss:\t\t0.522040\n",
      "  validation loss:\t\t0.523067\n",
      "Epoch 255 of 500 took 13.271s\n",
      "  training loss:\t\t0.522023\n",
      "  validation loss:\t\t0.523959\n",
      "Epoch 256 of 500 took 13.272s\n",
      "  training loss:\t\t0.522128\n",
      "  validation loss:\t\t0.523100\n",
      "Epoch 257 of 500 took 13.273s\n",
      "  training loss:\t\t0.521948\n",
      "  validation loss:\t\t0.523488\n",
      "Epoch 258 of 500 took 13.265s\n",
      "  training loss:\t\t0.522170\n",
      "  validation loss:\t\t0.523198\n",
      "Epoch 259 of 500 took 13.273s\n",
      "  training loss:\t\t0.521926\n",
      "  validation loss:\t\t0.523143\n",
      "Epoch 260 of 500 took 13.268s\n",
      "  training loss:\t\t0.522183\n",
      "  validation loss:\t\t0.523790\n",
      "Epoch 261 of 500 took 13.271s\n",
      "  training loss:\t\t0.521932\n",
      "  validation loss:\t\t0.523258\n",
      "Epoch 262 of 500 took 13.267s\n",
      "  training loss:\t\t0.522286\n",
      "  validation loss:\t\t0.523781\n",
      "Epoch 263 of 500 took 13.264s\n",
      "  training loss:\t\t0.521968\n",
      "  validation loss:\t\t0.523513\n",
      "Epoch 264 of 500 took 13.268s\n",
      "  training loss:\t\t0.522052\n",
      "  validation loss:\t\t0.523815\n",
      "Epoch 265 of 500 took 13.266s\n",
      "  training loss:\t\t0.521963\n",
      "  validation loss:\t\t0.523428\n",
      "Epoch 266 of 500 took 13.263s\n",
      "  training loss:\t\t0.522022\n",
      "  validation loss:\t\t0.523866\n",
      "Epoch 267 of 500 took 13.268s\n",
      "  training loss:\t\t0.521938\n",
      "  validation loss:\t\t0.523281\n",
      "Epoch 268 of 500 took 13.273s\n",
      "  training loss:\t\t0.522022\n",
      "  validation loss:\t\t0.524045\n",
      "Epoch 269 of 500 took 13.265s\n",
      "  training loss:\t\t0.521900\n",
      "  validation loss:\t\t0.523048\n",
      "Epoch 270 of 500 took 13.266s\n",
      "  training loss:\t\t0.521777\n",
      "  validation loss:\t\t0.523161\n",
      "Epoch 271 of 500 took 13.304s\n",
      "  training loss:\t\t0.522247\n",
      "  validation loss:\t\t0.523725\n",
      "Epoch 272 of 500 took 13.265s\n",
      "  training loss:\t\t0.521822\n",
      "  validation loss:\t\t0.523138\n",
      "Epoch 273 of 500 took 13.265s\n",
      "  training loss:\t\t0.522149\n",
      "  validation loss:\t\t0.523292\n",
      "Epoch 274 of 500 took 13.266s\n",
      "  training loss:\t\t0.521891\n",
      "  validation loss:\t\t0.523205\n",
      "Epoch 275 of 500 took 13.276s\n",
      "  training loss:\t\t0.521945\n",
      "  validation loss:\t\t0.523333\n",
      "Epoch 276 of 500 took 13.269s\n",
      "  training loss:\t\t0.521901\n",
      "  validation loss:\t\t0.523117\n",
      "Epoch 277 of 500 took 13.267s\n",
      "  training loss:\t\t0.521916\n",
      "  validation loss:\t\t0.523442\n",
      "Epoch 278 of 500 took 13.265s\n",
      "  training loss:\t\t0.521888\n",
      "  validation loss:\t\t0.523008\n",
      "Epoch 279 of 500 took 13.270s\n",
      "  training loss:\t\t0.521873\n",
      "  validation loss:\t\t0.523636\n",
      "Epoch 280 of 500 took 13.288s\n",
      "  training loss:\t\t0.521905\n",
      "  validation loss:\t\t0.522944\n",
      "Epoch 281 of 500 took 13.265s\n",
      "  training loss:\t\t0.521700\n",
      "  validation loss:\t\t0.522879\n",
      "Epoch 282 of 500 took 13.270s\n",
      "  training loss:\t\t0.521749\n",
      "  validation loss:\t\t0.523944\n",
      "Epoch 283 of 500 took 13.273s\n",
      "  training loss:\t\t0.522132\n",
      "  validation loss:\t\t0.522911\n",
      "Epoch 284 of 500 took 13.282s\n",
      "  training loss:\t\t0.521881\n",
      "  validation loss:\t\t0.524817\n",
      "Epoch 285 of 500 took 13.272s\n",
      "  training loss:\t\t0.522001\n",
      "  validation loss:\t\t0.523409\n",
      "Epoch 286 of 500 took 13.266s\n",
      "  training loss:\t\t0.521863\n",
      "  validation loss:\t\t0.523506\n",
      "Epoch 287 of 500 took 13.272s\n",
      "  training loss:\t\t0.521837\n",
      "  validation loss:\t\t0.523369\n",
      "Epoch 288 of 500 took 13.273s\n",
      "  training loss:\t\t0.521840\n",
      "  validation loss:\t\t0.523515\n",
      "Epoch 289 of 500 took 13.272s\n",
      "  training loss:\t\t0.521810\n",
      "  validation loss:\t\t0.523278\n",
      "Epoch 290 of 500 took 13.275s\n",
      "  training loss:\t\t0.521837\n",
      "  validation loss:\t\t0.523632\n",
      "Epoch 291 of 500 took 13.271s\n",
      "  training loss:\t\t0.521765\n",
      "  validation loss:\t\t0.523046\n",
      "Epoch 292 of 500 took 13.258s\n",
      "  training loss:\t\t0.521808\n",
      "  validation loss:\t\t0.523940\n",
      "Epoch 293 of 500 took 13.269s\n",
      "  training loss:\t\t0.521792\n",
      "  validation loss:\t\t0.523000\n",
      "Epoch 294 of 500 took 13.295s\n",
      "  training loss:\t\t0.521653\n",
      "  validation loss:\t\t0.523170\n",
      "Epoch 295 of 500 took 13.267s\n",
      "  training loss:\t\t0.522028\n",
      "  validation loss:\t\t0.523540\n",
      "Epoch 296 of 500 took 13.275s\n",
      "  training loss:\t\t0.521692\n",
      "  validation loss:\t\t0.523465\n",
      "Epoch 297 of 500 took 13.277s\n",
      "  training loss:\t\t0.521979\n",
      "  validation loss:\t\t0.523165\n",
      "Epoch 298 of 500 took 13.290s\n",
      "  training loss:\t\t0.521748\n",
      "  validation loss:\t\t0.523167\n",
      "Epoch 299 of 500 took 13.269s\n",
      "  training loss:\t\t0.521775\n",
      "  validation loss:\t\t0.523168\n",
      "Epoch 300 of 500 took 13.276s\n",
      "  training loss:\t\t0.521737\n",
      "  validation loss:\t\t0.523056\n",
      "Epoch 301 of 500 took 13.275s\n",
      "  training loss:\t\t0.521753\n",
      "  validation loss:\t\t0.523241\n",
      "Epoch 302 of 500 took 13.272s\n",
      "  training loss:\t\t0.521721\n",
      "  validation loss:\t\t0.522961\n",
      "Epoch 303 of 500 took 13.277s\n",
      "  training loss:\t\t0.521737\n",
      "  validation loss:\t\t0.523365\n",
      "Epoch 304 of 500 took 13.264s\n",
      "  training loss:\t\t0.521706\n",
      "  validation loss:\t\t0.522845\n",
      "Epoch 305 of 500 took 13.276s\n",
      "  training loss:\t\t0.521684\n",
      "  validation loss:\t\t0.523517\n",
      "Epoch 306 of 500 took 13.266s\n",
      "  training loss:\t\t0.521743\n",
      "  validation loss:\t\t0.522848\n",
      "Epoch 307 of 500 took 13.276s\n",
      "  training loss:\t\t0.521589\n",
      "  validation loss:\t\t0.523021\n",
      "Epoch 308 of 500 took 13.262s\n",
      "  training loss:\t\t0.521836\n",
      "  validation loss:\t\t0.523209\n",
      "Epoch 309 of 500 took 13.273s\n",
      "  training loss:\t\t0.521587\n",
      "  validation loss:\t\t0.522703\n",
      "Epoch 310 of 500 took 13.267s\n",
      "  training loss:\t\t0.521980\n",
      "  validation loss:\t\t0.523557\n",
      "Epoch 311 of 500 took 13.273s\n",
      "  training loss:\t\t0.521576\n",
      "  validation loss:\t\t0.522863\n",
      "Epoch 312 of 500 took 13.269s\n",
      "  training loss:\t\t0.521714\n",
      "  validation loss:\t\t0.523585\n",
      "Epoch 313 of 500 took 13.268s\n",
      "  training loss:\t\t0.521621\n",
      "  validation loss:\t\t0.522920\n",
      "Epoch 314 of 500 took 13.273s\n",
      "  training loss:\t\t0.521678\n",
      "  validation loss:\t\t0.523561\n",
      "Epoch 315 of 500 took 13.273s\n",
      "  training loss:\t\t0.521599\n",
      "  validation loss:\t\t0.522825\n",
      "Epoch 316 of 500 took 13.295s\n",
      "  training loss:\t\t0.521629\n",
      "  validation loss:\t\t0.523744\n",
      "Epoch 317 of 500 took 13.296s\n",
      "  training loss:\t\t0.521653\n",
      "  validation loss:\t\t0.522925\n",
      "Epoch 318 of 500 took 13.275s\n",
      "  training loss:\t\t0.521553\n",
      "  validation loss:\t\t0.523249\n",
      "Epoch 319 of 500 took 13.267s\n",
      "  training loss:\t\t0.521719\n",
      "  validation loss:\t\t0.523295\n",
      "Epoch 320 of 500 took 13.274s\n",
      "  training loss:\t\t0.521485\n",
      "  validation loss:\t\t0.522621\n",
      "Epoch 321 of 500 took 13.282s\n",
      "  training loss:\t\t0.521881\n",
      "  validation loss:\t\t0.523456\n",
      "Epoch 322 of 500 took 13.276s\n",
      "  training loss:\t\t0.521516\n",
      "  validation loss:\t\t0.522748\n",
      "Epoch 323 of 500 took 13.276s\n",
      "  training loss:\t\t0.521623\n",
      "  validation loss:\t\t0.523287\n",
      "Epoch 324 of 500 took 13.266s\n",
      "  training loss:\t\t0.521553\n",
      "  validation loss:\t\t0.522780\n",
      "Epoch 325 of 500 took 13.273s\n",
      "  training loss:\t\t0.521569\n",
      "  validation loss:\t\t0.523188\n",
      "Epoch 326 of 500 took 13.277s\n",
      "  training loss:\t\t0.521548\n",
      "  validation loss:\t\t0.522763\n",
      "Epoch 327 of 500 took 13.272s\n",
      "  training loss:\t\t0.521531\n",
      "  validation loss:\t\t0.523196\n",
      "Epoch 328 of 500 took 13.265s\n",
      "  training loss:\t\t0.521553\n",
      "  validation loss:\t\t0.522748\n",
      "Epoch 329 of 500 took 13.294s\n",
      "  training loss:\t\t0.521479\n",
      "  validation loss:\t\t0.523097\n",
      "Epoch 330 of 500 took 13.274s\n",
      "  training loss:\t\t0.521582\n",
      "  validation loss:\t\t0.522830\n",
      "Epoch 331 of 500 took 13.273s\n",
      "  training loss:\t\t0.521427\n",
      "  validation loss:\t\t0.522714\n",
      "Epoch 332 of 500 took 13.268s\n",
      "  training loss:\t\t0.521619\n",
      "  validation loss:\t\t0.523474\n",
      "Epoch 333 of 500 took 13.270s\n",
      "  training loss:\t\t0.521429\n",
      "  validation loss:\t\t0.522712\n",
      "Epoch 334 of 500 took 13.282s\n",
      "  training loss:\t\t0.521744\n",
      "  validation loss:\t\t0.523242\n",
      "Epoch 335 of 500 took 13.271s\n",
      "  training loss:\t\t0.521425\n",
      "  validation loss:\t\t0.522819\n",
      "Epoch 336 of 500 took 13.276s\n",
      "  training loss:\t\t0.521518\n",
      "  validation loss:\t\t0.523257\n",
      "Epoch 337 of 500 took 13.263s\n",
      "  training loss:\t\t0.521428\n",
      "  validation loss:\t\t0.522765\n",
      "Epoch 338 of 500 took 13.269s\n",
      "  training loss:\t\t0.521485\n",
      "  validation loss:\t\t0.523341\n",
      "Epoch 339 of 500 took 13.287s\n",
      "  training loss:\t\t0.521409\n",
      "  validation loss:\t\t0.522651\n",
      "Epoch 340 of 500 took 13.272s\n",
      "  training loss:\t\t0.521394\n",
      "  validation loss:\t\t0.523354\n",
      "Epoch 341 of 500 took 13.266s\n",
      "  training loss:\t\t0.521514\n",
      "  validation loss:\t\t0.522928\n",
      "Epoch 342 of 500 took 13.262s\n",
      "  training loss:\t\t0.521318\n",
      "  validation loss:\t\t0.522557\n",
      "Epoch 343 of 500 took 13.274s\n",
      "  training loss:\t\t0.521291\n",
      "  validation loss:\t\t0.523036\n",
      "Epoch 344 of 500 took 13.268s\n",
      "  training loss:\t\t0.521646\n",
      "  validation loss:\t\t0.523013\n",
      "Epoch 345 of 500 took 13.277s\n",
      "  training loss:\t\t0.521300\n",
      "  validation loss:\t\t0.522979\n",
      "Epoch 346 of 500 took 13.267s\n",
      "  training loss:\t\t0.521636\n",
      "  validation loss:\t\t0.522900\n",
      "Epoch 347 of 500 took 13.273s\n",
      "  training loss:\t\t0.521357\n",
      "  validation loss:\t\t0.522818\n",
      "Epoch 348 of 500 took 13.278s\n",
      "  training loss:\t\t0.521406\n",
      "  validation loss:\t\t0.522906\n",
      "Epoch 349 of 500 took 13.268s\n",
      "  training loss:\t\t0.521345\n",
      "  validation loss:\t\t0.522732\n",
      "Epoch 350 of 500 took 13.264s\n",
      "  training loss:\t\t0.521376\n",
      "  validation loss:\t\t0.522959\n",
      "Epoch 351 of 500 took 13.271s\n",
      "  training loss:\t\t0.521327\n",
      "  validation loss:\t\t0.522630\n",
      "Epoch 352 of 500 took 13.273s\n",
      "  training loss:\t\t0.521340\n",
      "  validation loss:\t\t0.523072\n",
      "Epoch 353 of 500 took 13.270s\n",
      "  training loss:\t\t0.521335\n",
      "  validation loss:\t\t0.522619\n",
      "Epoch 354 of 500 took 13.276s\n",
      "  training loss:\t\t0.521275\n",
      "  validation loss:\t\t0.522916\n",
      "Epoch 355 of 500 took 13.265s\n",
      "  training loss:\t\t0.521387\n",
      "  validation loss:\t\t0.522812\n",
      "Epoch 356 of 500 took 13.277s\n",
      "  training loss:\t\t0.521209\n",
      "  validation loss:\t\t0.522401\n",
      "Epoch 357 of 500 took 13.271s\n",
      "  training loss:\t\t0.521115\n",
      "  validation loss:\t\t0.522515\n",
      "Epoch 358 of 500 took 13.271s\n",
      "  training loss:\t\t0.521645\n",
      "  validation loss:\t\t0.522878\n",
      "Epoch 359 of 500 took 13.266s\n",
      "  training loss:\t\t0.521214\n",
      "  validation loss:\t\t0.522647\n",
      "Epoch 360 of 500 took 13.270s\n",
      "  training loss:\t\t0.521335\n",
      "  validation loss:\t\t0.523124\n",
      "Epoch 361 of 500 took 13.278s\n",
      "  training loss:\t\t0.521203\n",
      "  validation loss:\t\t0.522403\n",
      "Epoch 362 of 500 took 13.278s\n",
      "  training loss:\t\t0.521191\n",
      "  validation loss:\t\t0.523256\n",
      "Epoch 363 of 500 took 13.302s\n",
      "  training loss:\t\t0.521336\n",
      "  validation loss:\t\t0.522709\n",
      "Epoch 364 of 500 took 13.274s\n",
      "  training loss:\t\t0.521136\n",
      "  validation loss:\t\t0.522421\n",
      "Epoch 365 of 500 took 13.281s\n",
      "  training loss:\t\t0.521311\n",
      "  validation loss:\t\t0.523585\n",
      "Epoch 366 of 500 took 13.270s\n",
      "  training loss:\t\t0.521174\n",
      "  validation loss:\t\t0.522492\n",
      "Epoch 367 of 500 took 13.271s\n",
      "  training loss:\t\t0.521455\n",
      "  validation loss:\t\t0.522816\n",
      "Epoch 368 of 500 took 13.277s\n",
      "  training loss:\t\t0.521151\n",
      "  validation loss:\t\t0.522587\n",
      "Epoch 369 of 500 took 13.263s\n",
      "  training loss:\t\t0.521226\n",
      "  validation loss:\t\t0.522807\n",
      "Epoch 370 of 500 took 13.274s\n",
      "  training loss:\t\t0.521151\n",
      "  validation loss:\t\t0.522555\n",
      "Epoch 371 of 500 took 13.269s\n",
      "  training loss:\t\t0.521189\n",
      "  validation loss:\t\t0.522793\n",
      "Epoch 372 of 500 took 13.272s\n",
      "  training loss:\t\t0.521125\n",
      "  validation loss:\t\t0.522474\n",
      "Epoch 373 of 500 took 13.269s\n",
      "  training loss:\t\t0.521166\n",
      "  validation loss:\t\t0.522871\n",
      "Epoch 374 of 500 took 13.274s\n",
      "  training loss:\t\t0.521102\n",
      "  validation loss:\t\t0.522349\n",
      "Epoch 375 of 500 took 13.271s\n",
      "  training loss:\t\t0.521098\n",
      "  validation loss:\t\t0.522982\n",
      "Epoch 376 of 500 took 13.269s\n",
      "  training loss:\t\t0.521147\n",
      "  validation loss:\t\t0.522374\n",
      "Epoch 377 of 500 took 13.272s\n",
      "  training loss:\t\t0.520999\n",
      "  validation loss:\t\t0.522447\n",
      "Epoch 378 of 500 took 13.269s\n",
      "  training loss:\t\t0.521235\n",
      "  validation loss:\t\t0.522757\n",
      "Epoch 379 of 500 took 13.287s\n",
      "  training loss:\t\t0.520969\n",
      "  validation loss:\t\t0.522185\n",
      "Epoch 380 of 500 took 13.266s\n",
      "  training loss:\t\t0.521335\n",
      "  validation loss:\t\t0.522961\n",
      "Epoch 381 of 500 took 13.274s\n",
      "  training loss:\t\t0.520989\n",
      "  validation loss:\t\t0.522342\n",
      "Epoch 382 of 500 took 13.264s\n",
      "  training loss:\t\t0.521091\n",
      "  validation loss:\t\t0.522981\n",
      "Epoch 383 of 500 took 13.285s\n",
      "  training loss:\t\t0.520994\n",
      "  validation loss:\t\t0.522333\n",
      "Epoch 384 of 500 took 13.270s\n",
      "  training loss:\t\t0.521047\n",
      "  validation loss:\t\t0.522983\n",
      "Epoch 385 of 500 took 13.285s\n",
      "  training loss:\t\t0.520974\n",
      "  validation loss:\t\t0.522261\n",
      "Epoch 386 of 500 took 13.296s\n",
      "  training loss:\t\t0.520976\n",
      "  validation loss:\t\t0.522999\n",
      "Epoch 387 of 500 took 13.269s\n",
      "  training loss:\t\t0.521014\n",
      "  validation loss:\t\t0.522388\n",
      "Epoch 388 of 500 took 13.277s\n",
      "  training loss:\t\t0.520899\n",
      "  validation loss:\t\t0.522476\n",
      "Epoch 389 of 500 took 13.267s\n",
      "  training loss:\t\t0.521079\n",
      "  validation loss:\t\t0.522848\n",
      "Epoch 390 of 500 took 13.277s\n",
      "  training loss:\t\t0.520847\n",
      "  validation loss:\t\t0.522119\n",
      "Epoch 391 of 500 took 13.272s\n",
      "  training loss:\t\t0.521171\n",
      "  validation loss:\t\t0.522567\n",
      "Epoch 392 of 500 took 13.271s\n",
      "  training loss:\t\t0.520881\n",
      "  validation loss:\t\t0.522241\n",
      "Epoch 393 of 500 took 13.273s\n",
      "  training loss:\t\t0.520936\n",
      "  validation loss:\t\t0.522515\n",
      "Epoch 394 of 500 took 13.280s\n",
      "  training loss:\t\t0.520881\n",
      "  validation loss:\t\t0.522183\n",
      "Epoch 395 of 500 took 13.272s\n",
      "  training loss:\t\t0.520881\n",
      "  validation loss:\t\t0.522519\n",
      "Epoch 396 of 500 took 13.266s\n",
      "  training loss:\t\t0.520871\n",
      "  validation loss:\t\t0.522137\n",
      "Epoch 397 of 500 took 13.279s\n",
      "  training loss:\t\t0.520824\n",
      "  validation loss:\t\t0.522483\n",
      "Epoch 398 of 500 took 13.264s\n",
      "  training loss:\t\t0.520892\n",
      "  validation loss:\t\t0.522213\n",
      "Epoch 399 of 500 took 13.269s\n",
      "  training loss:\t\t0.520769\n",
      "  validation loss:\t\t0.522111\n",
      "Epoch 400 of 500 took 13.265s\n",
      "  training loss:\t\t0.520894\n",
      "  validation loss:\t\t0.522692\n",
      "Epoch 401 of 500 took 13.274s\n",
      "  training loss:\t\t0.520754\n",
      "  validation loss:\t\t0.521864\n",
      "Epoch 402 of 500 took 13.278s\n",
      "  training loss:\t\t0.521043\n",
      "  validation loss:\t\t0.522927\n",
      "Epoch 403 of 500 took 13.273s\n",
      "  training loss:\t\t0.520717\n",
      "  validation loss:\t\t0.522002\n",
      "Epoch 404 of 500 took 13.273s\n",
      "  training loss:\t\t0.520814\n",
      "  validation loss:\t\t0.522718\n",
      "Epoch 405 of 500 took 13.269s\n",
      "  training loss:\t\t0.520733\n",
      "  validation loss:\t\t0.522054\n",
      "Epoch 406 of 500 took 13.276s\n",
      "  training loss:\t\t0.520765\n",
      "  validation loss:\t\t0.522645\n",
      "Epoch 407 of 500 took 13.268s\n",
      "  training loss:\t\t0.520710\n",
      "  validation loss:\t\t0.522013\n",
      "Epoch 408 of 500 took 13.282s\n",
      "  training loss:\t\t0.520706\n",
      "  validation loss:\t\t0.522621\n",
      "Epoch 409 of 500 took 13.291s\n",
      "  training loss:\t\t0.520744\n",
      "  validation loss:\t\t0.522185\n",
      "Epoch 410 of 500 took 13.291s\n",
      "  training loss:\t\t0.520638\n",
      "  validation loss:\t\t0.522132\n",
      "Epoch 411 of 500 took 13.270s\n",
      "  training loss:\t\t0.520786\n",
      "  validation loss:\t\t0.522647\n",
      "Epoch 412 of 500 took 13.269s\n",
      "  training loss:\t\t0.520583\n",
      "  validation loss:\t\t0.521827\n",
      "Epoch 413 of 500 took 13.269s\n",
      "  training loss:\t\t0.520902\n",
      "  validation loss:\t\t0.522348\n",
      "Epoch 414 of 500 took 13.268s\n",
      "  training loss:\t\t0.520609\n",
      "  validation loss:\t\t0.521995\n",
      "Epoch 415 of 500 took 13.268s\n",
      "  training loss:\t\t0.520666\n",
      "  validation loss:\t\t0.522253\n",
      "Epoch 416 of 500 took 13.272s\n",
      "  training loss:\t\t0.520601\n",
      "  validation loss:\t\t0.521964\n",
      "Epoch 417 of 500 took 13.269s\n",
      "  training loss:\t\t0.520612\n",
      "  validation loss:\t\t0.522238\n",
      "Epoch 418 of 500 took 13.270s\n",
      "  training loss:\t\t0.520578\n",
      "  validation loss:\t\t0.521878\n",
      "Epoch 419 of 500 took 13.283s\n",
      "  training loss:\t\t0.520567\n",
      "  validation loss:\t\t0.522275\n",
      "Epoch 420 of 500 took 13.276s\n",
      "  training loss:\t\t0.520571\n",
      "  validation loss:\t\t0.521820\n",
      "Epoch 421 of 500 took 13.265s\n",
      "  training loss:\t\t0.520504\n",
      "  validation loss:\t\t0.522193\n",
      "Epoch 422 of 500 took 13.265s\n",
      "  training loss:\t\t0.520586\n",
      "  validation loss:\t\t0.521911\n",
      "Epoch 423 of 500 took 13.265s\n",
      "  training loss:\t\t0.520444\n",
      "  validation loss:\t\t0.521771\n",
      "Epoch 424 of 500 took 13.287s\n",
      "  training loss:\t\t0.520580\n",
      "  validation loss:\t\t0.522496\n",
      "Epoch 425 of 500 took 13.261s\n",
      "  training loss:\t\t0.520437\n",
      "  validation loss:\t\t0.521591\n",
      "Epoch 426 of 500 took 13.275s\n",
      "  training loss:\t\t0.520758\n",
      "  validation loss:\t\t0.522352\n",
      "Epoch 427 of 500 took 13.266s\n",
      "  training loss:\t\t0.520397\n",
      "  validation loss:\t\t0.521760\n",
      "Epoch 428 of 500 took 13.274s\n",
      "  training loss:\t\t0.520494\n",
      "  validation loss:\t\t0.522275\n",
      "Epoch 429 of 500 took 13.266s\n",
      "  training loss:\t\t0.520406\n",
      "  validation loss:\t\t0.521781\n",
      "Epoch 430 of 500 took 13.272s\n",
      "  training loss:\t\t0.520443\n",
      "  validation loss:\t\t0.522243\n",
      "Epoch 431 of 500 took 13.284s\n",
      "  training loss:\t\t0.520368\n",
      "  validation loss:\t\t0.521671\n",
      "Epoch 432 of 500 took 13.297s\n",
      "  training loss:\t\t0.520399\n",
      "  validation loss:\t\t0.522385\n",
      "Epoch 433 of 500 took 13.276s\n",
      "  training loss:\t\t0.520361\n",
      "  validation loss:\t\t0.521613\n",
      "Epoch 434 of 500 took 13.272s\n",
      "  training loss:\t\t0.520250\n",
      "  validation loss:\t\t0.521878\n",
      "Epoch 435 of 500 took 13.268s\n",
      "  training loss:\t\t0.520534\n",
      "  validation loss:\t\t0.522105\n",
      "Epoch 436 of 500 took 13.271s\n",
      "  training loss:\t\t0.520217\n",
      "  validation loss:\t\t0.521583\n",
      "Epoch 437 of 500 took 13.269s\n",
      "  training loss:\t\t0.520554\n",
      "  validation loss:\t\t0.521967\n",
      "Epoch 438 of 500 took 13.272s\n",
      "  training loss:\t\t0.520262\n",
      "  validation loss:\t\t0.521683\n",
      "Epoch 439 of 500 took 13.271s\n",
      "  training loss:\t\t0.520317\n",
      "  validation loss:\t\t0.521924\n",
      "Epoch 440 of 500 took 13.264s\n",
      "  training loss:\t\t0.520249\n",
      "  validation loss:\t\t0.521617\n",
      "Epoch 441 of 500 took 13.269s\n",
      "  training loss:\t\t0.520271\n",
      "  validation loss:\t\t0.521917\n",
      "Epoch 442 of 500 took 13.272s\n",
      "  training loss:\t\t0.520228\n",
      "  validation loss:\t\t0.521551\n",
      "Epoch 443 of 500 took 13.266s\n",
      "  training loss:\t\t0.520229\n",
      "  validation loss:\t\t0.521986\n",
      "Epoch 444 of 500 took 13.271s\n",
      "  training loss:\t\t0.520209\n",
      "  validation loss:\t\t0.521454\n",
      "Epoch 445 of 500 took 13.275s\n",
      "  training loss:\t\t0.520146\n",
      "  validation loss:\t\t0.521880\n",
      "Epoch 446 of 500 took 13.268s\n",
      "  training loss:\t\t0.520256\n",
      "  validation loss:\t\t0.521622\n",
      "Epoch 447 of 500 took 13.269s\n",
      "  training loss:\t\t0.520093\n",
      "  validation loss:\t\t0.521397\n",
      "Epoch 448 of 500 took 13.271s\n",
      "  training loss:\t\t0.520218\n",
      "  validation loss:\t\t0.522292\n",
      "Epoch 449 of 500 took 13.273s\n",
      "  training loss:\t\t0.520094\n",
      "  validation loss:\t\t0.521220\n",
      "Epoch 450 of 500 took 13.264s\n",
      "  training loss:\t\t0.520396\n",
      "  validation loss:\t\t0.522071\n",
      "Epoch 451 of 500 took 13.277s\n",
      "  training loss:\t\t0.520050\n",
      "  validation loss:\t\t0.521369\n",
      "Epoch 452 of 500 took 13.267s\n",
      "  training loss:\t\t0.520136\n",
      "  validation loss:\t\t0.521968\n",
      "Epoch 453 of 500 took 13.288s\n",
      "  training loss:\t\t0.520075\n",
      "  validation loss:\t\t0.521430\n",
      "Epoch 454 of 500 took 13.272s\n",
      "  training loss:\t\t0.520072\n",
      "  validation loss:\t\t0.521842\n",
      "Epoch 455 of 500 took 13.295s"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-3b5fb2ffbd41>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(model, num_epochs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mtrain_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mtrain_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mloss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  training loss:\t\t0.520063\n",
      "  validation loss:\t\t0.521467\n"
     ]
    }
   ],
   "source": [
    " main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.array(loss_history).clip(max=3))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
